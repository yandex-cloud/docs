### Базовые принципы {#basic-principle}
**Интерконнект** добавляет акторной системе возможность передавать сообщения между **[акторами](actors.md)**, которые находятся в разных процессах операционной системы и/или на разных серверах.

В акторной системе есть два класса сообщений: локальные и нелокальные. Локальные сообщения всегда передаются в рамках одного процесса. Нелокальные сообщения могут быть переданы в другой процесс операционной системы, который может находиться как на том же сервере, так на другом сервере в сети. Передача нелокальных сообщений происходит относительно прозрачно для акторов. Для нелокальных сообщений требуется реализация функций сериализации и десериализации. В качестве основного способа сериализации используется protobuf (см. базовый класс [TEventPB](https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/core/actorlib/event_pb.h?op=blame&rev=2467069#l31)).

TActorID актора содержит номер "ноды", в которой находится актор. В рамках одного связанного **интерконнектом** кластера, номера всех нод уникальны. То есть каждому запущенному процессу соответствует свой уникальный номер "ноды".

При запуске акторной системы, для каждой соседней ноды акторная система создает сервисный актор [TInterconnectProxyTCP](https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/core/actorlib_impl/interconnect_tcp_proxy.h?op=blame&rev=2443878#l14). Все сообщения, которые нужно передать на другую ноду попадают в соответствующий актор-proxy. Как только у прокси возникает необходимость передать сообщения на другую ноду, он запускает процесс создания сессии **интерконнекта** между двумя нодами. В ходе этого процесса создаётся tcp соединение и акторы сессии [TInterconnectSessionTCP](https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/core/actorlib_impl/interconnect_tcp_session.h?op=blame&rev=2467069#l33). Актор сессии работает на том же mailbox, что и актор прокси.

Между двумя процессами **интерконнект** всегда создаёт только одну сессию. Для каждой сессии **интерконнект** создаёт одно tcp соединение. В рамках одной сессии **интерконнект** может создать  произвольное количество каналов (но не более 65536). В рамках одного канала **интерконнект** доставляет сообщения в том порядке, в котором сообщения поступили актору **интерконнекта**. Для каналов можно указать соотношение объёма передаваемого трафика - квоты. Канал для отправки сообщения указывается отправителем сообщения.

В случае если сетевая связность не нарушается, то гарантии **интерконнекта** следующие:

- **Интерконнект** доставляет сообщение, то есть не выбрасывает произвольные сообщения.
- Все соообщения в рамках одного канала доставляются в том порядке, в котором они поступили актору **интерконнекта**.
- Порядок доставки двух сообщений в двух разных каналах может быть произвольным.
- Сериализация сообщений происходит в процессе выполнения актора **интерконнекта**.
- Конечная десериализация сообщений происходит не внутри **интерконнекта**, а в акторе конечного получателя сообщения.
- **Интерконнект** не следит за корректностью работы функций сериализации и десериализации.

В случае если сетевая связность нарушается, то гарантии **интерконнекта** следующие:

- В потоке сообщений от одного актора к другому, в рамках одного канала **интерконнект** доставляет сообщения в заданном порядке, но некоторые сообщения могут быть не доставлены.
- **Интерконнект** не может доставить сообщения дважды.
- Если отправитель подписался на сессию **интерконнекта**, то он получит сообщение о том, что нарушена сетевая связность.
- Если сообщение не было доставлено, то **интерконнект** постарается отправить сообщение актору, указанному в секции ForwardOnNondelivery, или отправителю, если поднят флаг FlagTrackDelivery, но без гарантий. Интерконнект  сообщит, что сообщение не было доставлено, если **интерконнекту** точно известно, что сообщение не доставлено.
- **Интерконнект** может терять сообщения в случае завершения сессии **интерконнекта**, и только в этом случае.

Флаг FlagForwardOnNondelivery сбрасывается при прохождении сообщения через **интерконнект**.

Чтобы контролировать возможным потери пакетов, **интерконнект** предоставляет следующие механизмы:

- Для сообщения в IEventHandle можно указать флаг FlagTrackDelivery или FlagForwardOnNondelivery. В случае потери связности все сообщения, которые находятся в очереди на отправку (которые не были полностью сериализованы и отправлен в сокет), будут обработаны с соответствие с этими флагами. То есть если у сообщения есть флаг FlagForwardOnNondelivery, то оно будет отправлено в адресу указанному в forwardOnNondelivery (см. конструктор IEventHandle). Если у сообщения есть флаг FlagTrackDelivery, то оно будет отправлено отправителю. Приоритет у флага FlagForwardOnNondelivery.
- Актор-отправитель может указать в IEventHandle флаг FlagSubscribeOnSession. **Интерконнект** поместит TActorID отправителя в свой список "подписчиков на сессию". В случае завершения сессии (а сообщения могут теряться только в случае завершения сессии) **интерконнект** отправит всем подписчиком уведомление о закрытии сессии. Получив такое уведомление, актор должен самостоятельно проконтролировать доставку отправленных сообщений.

#### С точки зрения конфигурирования {#from-a-configuration-point-of-view}
Полный список параметров конфигурации interconnect здесь (https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/driver/protos/config.proto):
```
message TInterconnectConfig {

    message TChannel {
        optional uint32 Index = 1;
        optional uint32 Quota = 2;
    }

    repeated TChannel Channel = 1;
    optional bool FirstTryBeforePoll = 2;
    optional bool StartTcp = 3 [default = false];
    optional uint32 SelfKickDelay = 4;
    optional uint32 HandshakeTimeout = 5;
    optional uint32 HeartbeatInterval = 6;
    optional uint32 DeadPeerTimeout = 7;
    optional uint32 SendBufferDieLimitInMB = 8;
}
```
Интересные параметры:
`SelfKickDelay` - виртуально, это таймаут на запись внутренних буферов в сокет в микросекундах. По умолчанию 0. Если выставить этот параметр 0 - то таймаута нет, и **интерконнект** будет всё сразу записывать в сокет. Такая агрессивная запись уменьшает средний размер tcp пакетов, увеличивает количество сисколов и увеличивает потребление CPU. Если хочется сэкономить немного CPU и пожертвовать для этого временем доставки сообщений, то можно выставить этот параметр в несколько сотен микросекунд. Здесь нужно помнить, что гранулярность таймаута равна гранулярности `Resolution` в шедулере акторной системы (см. message TActorSystemConfig и конфигурацию акторной системы).

`DeadPeerTimeout` - в миллисекундах, по-умолчанию 100 сек., если в течение указанного времени не было получено ни одного пакета от ноды, с которой установлено соединение, то эта нода считается мёртвой и **интерконнект** закрывает соединение. **Интерконнект** рассылает уведомления о закрытом соединении всем подписчикам.

`SendBufferDieLimitInMB` - в мегабайтах, по умолчанию 512. Если данных во внутренних буферах соединения становится больше, чем этот лимит, то **интерконнект** закрывает соединение и освобождает буферы. **Интерконнект** рассылает уведомления о закрытом соединении всем подписчикам.

Неинтересные параметры:

`HandshakeTimeout` - консолидированные таймаут на установку соединения между акторами **интерконнекта** в миллисекундах, туда входит создание сокета и обмен техническими сообщениями между акторами **интерконнекта**.

`HeartbeatInterval` - в миллисекундах как часто актор **интерконнекта** просыпается чтобы оценить состояние соединения. по умолчанию 10 секунд.

`FirstTryBeforePoll` - флажок, который позволяет оптимизировать работу **интерконнекта** на стадии отправки данных в сокет. уменьшает потребление CPU и время доставки данных. нет смысла не выставлять true. значение по-умолчанию - true.

`StartTcp` - флажок, который запускает tcp акторы.

#### С точки зрения программирования {#from-a-programming-point-of-view}
Чтобы передавать события между нодами, нужно одно из двух:

1) Наследовать событие от класса TEventPB и заполнить протобуф Record (см. https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/core/actorlib/event_pb.h)
2) Реализовать методы `Stroka Serialize() const override` и `static IEventBase* Load(TEventSerializedData* input)`.

Вызов метода `Load` происходит при получении события в конечном акторе-получателе.


### Как выкатывать изменения конфигурации кластера с помощью rolling update {#how-to-roll-out-cluster-configuration-changes-using-rolling-update}
#### Добавление серверов в кластер {#adding-servers-to-a-cluster}
Дано: Сервера {X}, из которых {Y} недоступны, а {X-Y} доступны. В ClusterUUID записана строчка SALT1. Кластер имеют версию конфигов C1. Машинки {Z}, которые необходимо добавить в кластер.
Действие 1: Сформировать версию конфигов C2 следующим образом: взять версию конфигов C1, добавить в эту версию новые сервера, в ClusterUUID записать SALT2, в AcceptUUID записать SALT1
Действие 2: Выкатить версию конфигов C2 на машинки {X-Y}
Действие 3: Сформировать версию конфигов C3 следующим образом: взять версию конфигов C2, убрать запись AcceptUUID.
Действие 4: Выкатить версию конфигов С3 на машинки {X-Y}
Действие 5: Выкатить версию конфигов С3 на машинки {Z}

#### Удаление серверов из кластера {#removing-servers-from-the-cluster}
Дано: Сервера {X} в кластере. Нужно исключить из кластера машинки {Y}, причём эти машинки недоступны. В ClusterUUID записана строчка SALT1. Кластер имеют версию конфигов C1.
Действие 1: Сформировать версию конфигов C2, в которой исключены машинки {Y}. В ClusterUUID записать SALT2, В AcceptUUID записать SALT1.
Действие 2: Удалить внутренние ресурсы kikimr (таблетки, blob-storage) с машинок {Y} , запретить выделение и запуск новых ресурсов на машинках {Y}.
Действие 3: Выкатить версию конфигов C2 на машинки {X-Y}.

### Информация для разработчика {#information-for-the-developer}
#### Создание сессии {#creating-a-session}
При инициализации акторной системы для каждой соседней ноды создаётся объект TInterconnectProxyTCP, также создаётся два потока для поллера (linux epoll), один поток для ожидания записи данных, второй поток для ожидания чтения данных.
Когда TInterconnectProxyTCP получает сообщение из акторной системы, которое нужно передать на соседнюю ноду, в этот момент TInterconnectProxyTCP запускает процесс создания tcp сессии - создаёт актор THandshake с режиме Outgoing.
THandshake спрашивает ip адрес у актора DNS кеша (см. THandshake::SendDnsRequest).
THandshake, получив ip адрес, создаёт сокет и с помощью поллера устанавливает соединение.
После установке tcp соединения THandshake передаёт в сокет структуру THandshake::TData, после этого читает из сокета такую же структуру в ответ. Если в структуре указана версия протокола 2 или больше, то дополнительно происходит обмен протобуфами, которые содержат дополнительные данные (см. THandshake::ReadExtendedOptions и THandshake::WriteExtendedOptions).
