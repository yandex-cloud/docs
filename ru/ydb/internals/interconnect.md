## Интерконнект - передача сообщений по сети в библиотеке actors
### Обзор
**Интерконнект** добавляет акторной системе возможность передавать сообщения между **[акторами](actors.md)**, которые находятся в разных процессах операционной системы и/или на разных серверах.

В акторной системе есть два класса сообщений: локальные и нелокальные. Локальные сообщения всегда передаются в рамках одного процесса. Нелокальные сообщения могут быть переданы в другой процесс операционной системы, который может находиться как на том же сервере, так на другом сервере в сети. Передача нелокальных сообщений происходит относительно прозрачно для акторов. Для нелокальных сообщений требуется реализация функций сериализации и десериализации. В качестве основного способа сериализации используется protobuf (см. базовый класс `TEventPB`).

`TActorId` актора содержит номер ноды, в которой запущен актор. В рамках одного связанного **интерконнектом** кластера, номера всех нод уникальны. То есть каждому запущенному процессу соответствует свой уникальный номер ноды.

При запуске акторной системы, для каждой соседней ноды акторная система создает сервисный актор `TInterconnectProxyTCP`. Все сообщения, которые нужно передать на другую ноду попадают в соответствующий актор-прокси. Как только у прокси возникает необходимость передать сообщения на другую ноду, он запускает процесс создания сессии **интерконнекта** между двумя нодами. В ходе этого процесса создаётся tcp соединение и акторы сессии `TInterconnectSessionTCP`. Актор сессии работает на том же mailbox, что и актор прокси.

Между двумя процессами **интерконнект** всегда создаёт только одну сессию. Для каждой сессии **интерконнект** создаёт одно tcp соединение. В рамках одной сессии **интерконнект** может создать  произвольное количество каналов (но не более 65536). В рамках одного канала **интерконнект** доставляет сообщения в том порядке, в котором сообщения поступили актору **интерконнекта**. Для каналов можно указать соотношение объёма передаваемого трафика - квоты. Канал для отправки сообщения указывается отправителем сообщения.

В случае если сетевая связность не нарушается, то гарантии **интерконнекта** следующие:

- **Интерконнект** доставляет сообщение, то есть не выбрасывает произвольные сообщения.
- Все соообщения в рамках одного канала доставляются в том порядке, в котором они поступили актору **интерконнекта**.
- Порядок доставки двух сообщений в двух разных каналах может быть произвольным.
- Сериализация сообщений происходит в процессе выполнения актора **интерконнекта**.
- Конечная десериализация сообщений происходит не внутри **интерконнекта**, а в акторе конечного получателя сообщения.
- **Интерконнект** не следит за корректностью работы функций сериализации и десериализации.

В случае если сетевая связность нарушается, то гарантии **интерконнекта** следующие:

- В потоке сообщений от одного актора к другому, в рамках одного канала, **интерконнект** доставляет сообщения в заданном порядке, но некоторые сообщения могут быть не доставлены.
- **Интерконнект** не может доставить сообщения дважды.
- Если отправитель подписался на сессию **интерконнекта**, то он получит сообщение о том, что нарушена сетевая связность.
- Если сообщение не было доставлено, то **интерконнект** постарается отправить сообщение актору, указанному в секции ForwardOnNondelivery, или отправителю, если поднят флаг `FlagTrackDelivery`, но без гарантий. Интерконнект  сообщит, что сообщение не было доставлено, если **интерконнекту** точно известно, что сообщение не доставлено.
- **Интерконнект** может терять сообщения в случае завершения сессии **интерконнекта**, и только в этом случае.

Флаг `FlagForwardOnNondelivery` сбрасывается при прохождении сообщения через **интерконнект**.

Чтобы контролировать возможным потери сообщений, **интерконнект** предоставляет следующие механизмы:

 * Для сообщения в `IEventHandle` можно указать флаги `FlagTrackDelivery` и `FlagForwardOnNondelivery`. В случае потери связности все сообщения, которые находятся в очереди на отправку (то есть не были полностью сериализованы и отправлены в сокет), будут обработаны с соответствии с этими флагами:
   * Если у сообщения есть флаг `FlagForwardOnNondelivery`, то оно будет отправлено по адресу указанному в `forwardOnNondelivery` (см. конструктор `IEventHandle`);
   * Если у сообщения есть флаг `FlagTrackDelivery`, то оно будет отправлено отправителю. Приоритет у флага `FlagForwardOnNondelivery`;
 *  Актор-отправитель может указать в `IEventHandle` флаг `FlagSubscribeOnSession`. **Интерконнект** поместит `TActorId` отправителя в свой список "подписчиков на сессию". В случае завершения сессии (а сообщения могут теряться только в случае завершения сессии) **интерконнект** отправит всем подписчиком уведомление о закрытии сессии. Получив такое уведомление, актор имеет возможность обработать событие обрыва сессии.


### Интерконнект для пользователя библиотеки actors
#### Конфигурирование Интерконнекта
Полный список параметров конфигурации interconnect здесь (https://arc.yandex-team.ru/wsvn/arc/trunk/arcadia/kikimr/driver/protos/config.proto):
```
message TInterconnectConfig {

    message TChannel {
        optional uint32 Index = 1;
        optional uint32 Quota = 2;
    }

    repeated TChannel Channel = 1;
    optional bool FirstTryBeforePoll = 2;
    optional bool StartTcp = 3 [default = false];
    optional uint32 SelfKickDelay = 4;
    optional uint32 HandshakeTimeout = 5;
    optional uint32 HeartbeatInterval = 6;
    optional uint32 DeadPeerTimeout = 7;
    optional uint32 SendBufferDieLimitInMB = 8;
}
```
Интересные параметры:
`SelfKickDelay` - виртуально, это таймаут на запись внутренних буферов в сокет в микросекундах. По умолчанию 0. Если выставить этот параметр 0 - то таймаута нет, и **интерконнект** будет всё сразу записывать в сокет. Такая агрессивная запись уменьшает средний размер tcp пакетов, увеличивает количество сисколов и увеличивает потребление CPU. Если хочется сэкономить немного CPU и пожертвовать для этого временем доставки сообщений, то можно выставить этот параметр в несколько сотен микросекунд. Здесь нужно помнить, что гранулярность таймаута равна гранулярности `Resolution` в шедулере акторной системы (см. message TActorSystemConfig и конфигурацию акторной системы).

`DeadPeerTimeout` - в миллисекундах, по-умолчанию 100 сек., если в течение указанного времени не было получено ни одного пакета от ноды, с которой установлено соединение, то эта нода считается мёртвой и **интерконнект** закрывает соединение. **Интерконнект** рассылает уведомления о закрытом соединении всем подписчикам.

`SendBufferDieLimitInMB` - в мегабайтах, по умолчанию 512. Если данных во внутренних буферах соединения становится больше, чем этот лимит, то **интерконнект** закрывает соединение и освобождает буферы. **Интерконнект** рассылает уведомления о закрытом соединении всем подписчикам.

Неинтересные параметры:

`HandshakeTimeout` - консолидированные таймаут на установку соединения между акторами **интерконнекта** в миллисекундах, туда входит создание сокета и обмен техническими сообщениями между акторами **интерконнекта**.

`HeartbeatInterval` - в миллисекундах как часто актор **интерконнекта** просыпается чтобы оценить состояние соединения. по умолчанию 10 секунд.

`FirstTryBeforePoll` - флажок, который позволяет оптимизировать работу **интерконнекта** на стадии отправки данных в сокет. уменьшает потребление CPU и время доставки данных. нет смысла не выставлять true. значение по-умолчанию - true.

`StartTcp` - флажок, который запускает tcp акторы.

#### Сообщения передаваемые по Интерконнекту
Определить сообщение, которое пригодно для передачи между нодами, возможно следующими способами:

* Наследовать событие от класса `TEventPB`. При этом необходимо передать protobuf сообщение. Для сериализации и десериализации будут использованы возможности protobuf;
* Самому реализовать методы `Stroka Serialize() const override` и `static IEventBase* Load(TEventSerializedData* input)`.

Вызов метода `Load` происходит при получении события в конечном акторе-получателе.


### Заметки для DevOps приложений использующих Интерконнект
В данном разделе рассматривается последовательность действий для добавления/удаления машин в/из кластера с сохранением доступности и работоспособности кластера.

#### Добавление серверов в кластер 
Дано: Сервера {X}, из которых {Y} недоступны, а {X-Y} доступны. В ClusterUUID записана строчка SALT1. Кластер имеют версию конфигов C1. Машинки {Z}, которые необходимо добавить в кластер.
Действие 1: Сформировать версию конфигов C2 следующим образом: взять версию конфигов C1, добавить в эту версию новые сервера, в ClusterUUID записать SALT2, в AcceptUUID записать SALT1
Действие 2: Выкатить версию конфигов C2 на машинки {X-Y}
Действие 3: Сформировать версию конфигов C3 следующим образом: взять версию конфигов C2, убрать запись AcceptUUID.
Действие 4: Выкатить версию конфигов С3 на машинки {X-Y}
Действие 5: Выкатить версию конфигов С3 на машинки {Z}

#### Удаление серверов из кластера 
Дано: Сервера {X} в кластере. Нужно исключить из кластера машинки {Y}, причём эти машинки недоступны. В ClusterUUID записана строчка SALT1. Кластер имеют версию конфигов C1.
Действие 1: Сформировать версию конфигов C2, в которой исключены машинки {Y}. В ClusterUUID записать SALT2, В AcceptUUID записать SALT1.
Действие 2: Удалить внутренние ресурсы kikimr (таблетки, blob-storage) с машинок {Y} , запретить выделение и запуск новых ресурсов на машинках {Y}.
Действие 3: Выкатить версию конфигов C2 на машинки {X-Y}.

### Заметки для разработчика Интерконнекта
#### Создание сессии 
При инициализации акторной системы для каждой соседней ноды создаётся объект `TInterconnectProxyTCP`, также создаётся два потока для поллера (linux epoll), один поток для ожидания записи данных, второй поток для ожидания чтения данных. Когда `TInterconnectProxyTCP` получает сообщение из акторной системы, которое нужно передать на соседнюю ноду, в этот момент `TInterconnectProxyTCP` запускает процесс создания tcp сессии - создаёт актор `THandshake` в режиме `Outgoing`. `THandshake` спрашивает ip адрес у актора DNS кеша (см. `THandshake::SendDnsRequest`). `THandshake`, получив ip адрес, создаёт сокет и с помощью поллера устанавливает соединение.

После установки tcp соединения `THandshake` передаёт в сокет структуру `THandshake::TData`, после этого читает из сокета такую же структуру в ответ. Если в структуре указана версия протокола 2 или больше, то дополнительно происходит обмен протобуфами, которые содержат дополнительные данные (см. `THandshake::ReadExtendedOptions` и `THandshake::WriteExtendedOptions`).
