---
sourcePath: overlay/internals/actors.md
---
## Акторная модель и библиотека actors
**YDB** реализована при помощи библиотеки actors в рамках [Модели акторов](https://en.wikipedia.org/wiki/Actor_model).


### Мотивация и модель 

Современные базы данных - запутанный гетерогенный программный комплекс, отдельные части которого действуют независимо, взаимодействуют асинхронно, должны эффективно и прогнозируемо утилизировать доступные ресурсы - как отдельной массивно-многопроцессорной машины, так и кластера из сотен и тысяч узлов. Что ставит вопрос - а в какой, собственно, парадигме необходимо разрабатывать код, чтобы при таких вводных сохранить сложность разработки на разумном уровне.

В рамках проекта **YDB** была создана библиотека actors на C++. Логика каждого компонента представлена в виде явно выделенного конечного автомата реализованного в виде актора. Акторы обмениваются между собой сообщениями. Библиотека затачивалась под разработку **YDB**, но в конечном результате получилось сбалансированное, эффективное решение, которое можно смело рекомендовать как framework для построения сложных асинхронных программ, особенно если логика программы не укладывается в фиксированный линейный pipeline.

### Понятия Actor и ActorID 

Актор представляет собой специальный C++ класс, наследник IActor. Единица работы актора - обработка одиночного сообщения из входящей очереди (mailbox). В реализации актора специфицируется какие сообщения он может обрабатывать и какие обработчики вызывать на указанные сообщения.

В рамках обработки сообщения актор может:

  * изменить собственное состояние (как у C++ объекта у актора есть data members);
  * изменить функцию реакции (обработчик) на следующее событие;
  * послать сообщение другому актору, адрес которого ему известен;
  * создать новый актор;
  * умереть (только сам актор может решить умереть, невозможно убить актор снаружи);
  * взаимодействовать с внешней средой (например, передать обработку сторонней библиотеке).

Для актора гарантируется обработка не более одного события одновременно (т.е. с точки зрения отдельного актора обработка событий однопоточная) и упорядоченность обработки (любое сообщение отправленное позже завершения отправки предыдущего - не будет обработано в обратном порядке).

`ActorSystem` -- это пространство, где живут акторы. `ActorSystem` распространяется на несколько машин (например, на все машины кластера), в рамках одной `ActorSystem` акторы могут свободно общаться друг с другом посредством сообщений. Адресуются акторы с помощью структуры `TActorId`. С точки зрения внешнего наблюдателя - это адрес, который используется для адресации актора. `TActorId` назначается при регистрации нового актора в `ActorSystem` и должен формироваться строго определенным образом. Внутри содержит несколько полей:

  * `node-id` - число-номер ноды, на котором запущен данный актор. Может использоваться пользователем для определения локальности/удаленности адресата;
  * `local-id` - уникальное в рамках ноды число. Назначается при регистрации актора и не переиспользуется в рамках той же ноды. Обычно, не нужен пользователю, но может использоваться вместо полного адреса для идентификации локальных акторов;
  * `hint` - используется актор-системой для нахождения mailbox, к которому привязан актор (фактически - индекс в двухмерном массиве всех выделенных на ноде mailbox). Может переиспользоваться системой, для пользователя не несёт никакого смысла и не должен использоваться.

В комплексе `TActorId` является глобально уникальным в рамках одной инсталляции `ActorSystem` (`local-id` - уникален в рамках ноды, добавление `node-id` делает его глобально уникальным).

Актору собственный `TActorId` передается в каждом вызове обработки сообщения (как часть структуры `TActorContext`), так же `TActorId` новосозданного актора возвращается из функции регистрации актора (таким образом владелец актора может запомнить его для дальнейшего взаимодействия или идентификации).

### ServiceID 

`ServiceID` - это особый способ идентификации актора, который реализует сервис. Сервисы - это акторы порождаемые (обычно) на старте системы. Они формируют некоторое окружение, в рамках которого происходит работа приложения, динамически порождаются новые акторы и т.п. `ServiceID` представляет собой well known идентификатор сервиса.

 `ServiceID` - это, фактически, `TActorId`, но сформированный по особому правилу - вместо уникального `local-id` используется заданная бинарная строка длиной 12 байт. Вместе с `node-id` это позволяет адресовать сервисы в кластере, а при использовании специально `node-id` равного 0 - адресовать локальные сервисы, недоступные с других нод.

Строго говоря - `ServiceID` существует и без актора, который к нему привязан. Однако можно зарегистрировать (при создании акторсистемы или во время работы) трансляцию `ServiceID` в фактический `TActorId`. Логика обработки сообщения отправленного по `ServiceID` следующая:
 * если `node-id` указан ненулевой и несоответствующий текущей ноде, то сообщение форвардится в интерконнект;
 * если `node-id` нулевой или соответствующий текущей ноде, то выполняется поиск в таблице локальных зарегистрированных сервисов, и при нахождении адрес получателя перезаписывается с исходного `ServiceID` на фактический `TActorId`. Такая трансляция осуществляется не более одного раза (т.е. нельзя зарегистрировать `ServiceID` как другой `ServiceID`).

Основное предназначение `ServiceID` - связывание сервисов. Например, таким образом осуществляется доступ к локальным проксям BlobStorage, StateStorage.

### Декларация сообщений

Сообщение, передаваемое между акторами - это наследник класса `IEventBase`. Сообщение обязательно должно иметь тип - уникальное число типа `ui32`. Для компонента/набора компонент удобно выделить диапазон типов сообщений не пересекающихся с соседями (см. макросы `EventSpaceBegin/EventSpaceBegin` и их применение в коде).

Библиотека предоставляет два удобных класса-хелпера, от которых можно наследоваться при определении своих типов сообщений:

  * `TEventLocal` из `actors/core/event_local.h` - для событий, которые не должны покидать одну ноду, не определяют функции сериализации сообщения;
  * `TEventPB` из `actors/core/event_pb.h` - для событий, тело которых определено как protobuf-message, используются для сообщений, которые потенциально передаются между узлами.

Для особых случаев возможна частная реализация, метод сериализации никак не ограничивается со стороны библитеки.

### Отправка и доставка сообщений

При отправке сообщения `ActorSystem` оперирует объектами типа `IEventHandle`, которые содержат внутри следующие поля (перечислены только основные поля):

  * `ui32 Type` - тип сообщения;
  * `ui32 Flags` - флаги сообщения, используются, например, для отслеживания доставки сообщений и нотификаций о недоставке;
  * `TActorId Recipient` - адрес получателя;
  * `TActorId Sender` - адрес отправителя;
  * `ui64 Cookie` - неинтерпретируемое число, которое может использоваться, например, для дедупликации сообщений (семантику определяют отправитель/получатель);
  * объект-наследник `IEventBase` или буфер с сериализованным сообщением (`THolder<IEventBase> Event` или `TIntrusivePtr<TEventSerializedData> Buffer`) - собственно сообщение или сериализованное сообщение;
  * ряд дополнительных полей для трейсинга.

При посылке сообщения (`IEventHandle`) по `ActorID` адресата происходит следующее:
  * если получатель локален, то находится mailbox, к которому привязан актор-получатель (с возможной перезаписью, если исходный `ActorID` является `ServiceID`). Сообщение помещается во входящую очередь mailbox, а mailbox, если еще не активирован, активируется;
  * если получатель находится на удаленной ноде, то сообщение отправляется на актор интерконнекта, откуда пересылается на другой узел (с сериализацией сообщения).

При несуществовании запрошенного mailbox (или невозможности разыменовать `ServiceID`) происходит следующее:
  * если выставлено правило форварда, то сообщение переадресуется указанному актору;
  * иначе, если выставлен флаг отслеживания доставки - сообщение выбрасывается и отправителю отсылается сообщение-нотификация о недоставке от имена адресата;
  * иначе сообщение  выбрасывается.

Фактическая обработка сообщения произойдет позже, когда рабочий поток `ActorSystem` извлечёт сообщение из очереди. При этом будет совершена попытка найти на mailbox привязанный актор с требуемым `local-id` (его может не быть, если к моменту обработки сообщения актор уже умер). Если актор найден, то будет вызвана текущая установленная функция обработки сообщений. В противном случае логика обработки аналогична логике обработки ненайденного mailbox.

Существует возможность регистрации на один mailbox нескольких акторов. Сделать это можно только находясь внутри обработки сообщения актором и только на собственный mailbox. Созданные таким образом акторы разделяют общий mailbox, для них правило однопоточной обработки расширяется до всех акторов на одном mailbox. То есть акторы, работающие на одном mailbox, никогда не работают параллельно. Это дает возможность акторам на одном mailbox использовать одни и те же структуры данных без синхронизации.

С другой стороны стоит избегать порождения акторов на одном mailbox, т.к. степень параллельности программы резко снижается. Стоит отметить заметное количество инцидентов из-за того, что разработчик по ошибке порождал акторы на mailbox родителя, и система не смогла работать параллельно, когда возникала задача отмасштабироваться.

Способы запуска новых акторов:
 * `IActor::Register` - запуск актора на своем mailbox (должен использоваться в подавляющем большинстве случаев);
 * `IActor::RegisterLocal` - запуск актора на mailbox родителя (нерекомендуемый вариант, разработчик должен понимать что он делает и зачем).

При создании актора можно выбрать на каком типе mailbox будет создан актор. Доступно несколько типов:
  * Simple (`TMailboxType::EType::Simple`) - максимально облегченный mailbox, оптимизированный под минимальную цену посылки сообщения при отсутствии contention;
  * Revolving (`TMailbox::EType::Revolving`) - исторически рекламирующаяся как wait-free очередь; добавление сообщения в очередь ценой большей стоимости посылки/чтения; cледует использовать если предполагается существенный трафик на mailbox от лица разных отправителей. В настоящий момент критикуется;
  * HTSwap (`TMailbox::EType::HTSwap`) - текущее умолчание, lock-free очередь;
  * Есть также несколько других очередей.

Во всех типах mailbox писатели и читатель (да, именно так, у одного mailbox один читатель и несколько писателей) разведены и никогда не блокируют друг друга.

В рамках одной ноды кластера сообщения не сериализуются и передаются as-is, как поле `Event` в `IEventHandle`. При необходимости передачи между узлами, сообщение сериализуется в интерконнекте вызовом виртуальной функции `IEventBase::Serialize`. Обратный процесс, десериализация буфера в сообщение, должно осуществляться актором, который принимает сообщение. Поле `Type` в `IEventHandle` позволяет выбрать правильную функцию десериализации. Рекомендуемый способ - использовать макросы `HFunc`, `CFunc` и т.п. из `actors/core/hfunc.h`, тогда десериализация выполняется прозрачно для разработчика.


### Thread Pools

С точки зрения внутреннего устройства `ActorSystem` акторы - пассивные сущности, выполнение которых обеспечивается запуском обработки сообщений в контексте рабочих потоков пула потоков. Каждый mailbox (а с ним - и актор) привязан к конкретному пулу потоков и не может мигрировать в процессе жизни (но можно из актора одного пула потоков создать актор на соседнем пуле потоков). Пулы потоков нужны, чтобы изолировать по CPU определенные классы нагрузок, например, сейчас широко распространены следующие пулы потоков:
 * System - для нагрузки критически важных компонентов системы, например BlobStorage, StateStorage работают преимущественно в System Pool;
 * User - для пользовательской нагрузки, в этом пуле потоков выполняются запросы пользователей;
 * Batch - для нагрузки, которая не является критичной по времени ответа, например для background процессов; примером такой нагрузки является compaction таблеток и VDiskов;
 * IC (Interconnect Thread Pool) - явно выделенный пул потоков для сетевого взаимодействия.

Возможны разные варианты реализации пулов потоков, в имеющейся реализации количество потоков в пуле задаётся при старте системы и не меняется в процессе жизни. Обработка активаций mailbox'ов условно упорядочена (примерно одновременно активированные mailbox'ы будут примерно одновременно обработаны), привязки mailbox'ов к потокам выполнения не осуществляется и mailbox'ы не приоритезируются. Возможны другие реализации.

Базовый (basic-) пул потоков позволяет настраивать количество работающих потоков выполнения и время которое поток проводит в активном ожидании работы, прежде чем уснуть.

Стоит отметить, что правильным подходом к написанию обработчиков сообщений в акторах является такой, когда обработчик занимает CPU на крайне непродолжительное время. Если обработчик тяжелый, то надо уметь прерывать такой обработчик, например путем отправки сообщения себе же, либо отселять актор в Batch Pool.

### Шедулер 

Важным для реализации эффективного асинхронного кода является работа с временем и задержками. В `ActorSystem` шедулинг сообщений реализован через отложенную посылку сообщений. Основной сценарий - зашедулить посылку сообщения таймаута самому себе. Из коробки возможности отменить посылку сообщения нет (из соображений производительности), но возможно проигнорировать сообщение (в том числе и заранее, см. `ISchedulerCookie`).
