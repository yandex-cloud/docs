---
title: Взаимосвязь ресурсов сервиса {{ k8s }}
description: Основная сущность, которой оперирует сервис, — кластер {{ k8s }}. Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя.
---

# Взаимосвязь ресурсов в {{ managed-k8s-name }}


[{{ k8s }}](https://kubernetes.io/ru/) — система для управления [контейнерными приложениями](../../glossary/containerization.md#containers-apps). {{ k8s }} предоставляет механизмы взаимодействия с [кластером](../../glossary/cluster.md), с помощью которых осуществляется автоматизация развертывания, масштабирования и управления приложениями в контейнерах.

Основная сущность, которой оперирует сервис, — _кластер {{ k8s }}_.

## Кластер {{ k8s }} {#kubernetes-cluster}

Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя.

Сервис полностью управляет мастером, а также следит за состоянием и работоспособностью группы узлов. Пользователь может управлять узлами напрямую, а также настраивать кластер {{ k8s }} с помощью консоли управления {{ yandex-cloud }}, CLI и API {{ managed-k8s-name }}.

{% include [Install kubectl](../../_includes/managed-kubernetes/note-node-group-internet-access.md) %}

При работе с кластером {{ k8s }} на инфраструктуре {{ yandex-cloud }} задействуются следующие ресурсы:

Ресурс | Количество | Комментарий
--- | --- | ---
Подсеть | 2 | {{ k8s }} резервирует диапазоны IP-адресов, которые будут использоваться для подов и сервисов.
Публичный IP | N | В количество N входит:<br>* **Один** публичный IP для NAT-инстанса.<br>* Публичный IP **каждому** узлу в группе, если вы используете технологию one-to-one NAT.

### Метки кластера {#cluster-labels}

Для разделения кластеров {{ k8s }} по логическим группам используйте [облачные метки](../../resource-manager/concepts/labels.md).

Облачные метки для кластеров {{ k8s }} составляются по следующим правилам:

  {% include [cloud-labels-restrictions-cluster](../../_includes/managed-kubernetes/cloud-labels-restrictions-cluster.md) %}

Об управлении облачными метками читайте в разделе [Изменение кластера](../operations/kubernetes-cluster/kubernetes-cluster-update.md#manage-label).

{% note info %}

Кластеру нельзя назначить [{{ k8s }}-метки]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/).

{% endnote %}

## Мастер {#master}

_Мастер_ — компонент, который управляет кластером {{ k8s }}.

Мастер запускает управляющие процессы {{ k8s }}, которые включают сервер {{ k8s }} API, планировщик и контроллеры основных ресурсов. Жизненный цикл мастера управляется сервисом при создании или удалении кластера {{ k8s }}. Мастер отвечает за глобальные решения, которые выполняются на всех узлах кластера {{ k8s }}. Они включают в себя планирование рабочих нагрузок, таких как контейнерные приложения, управление жизненным циклом рабочих нагрузок и масштабированием.

Мастер бывает следующих типов, которые отличаются количеством хостов мастера и размещением в [зонах доступности](../../overview/concepts/geo-scope.md):
* _Базовый_ — содержит один хост мастера в одной зоне доступности. Такой мастер дешевле, но он не является отказоустойчивым. Прежнее название — _зональный_.

  {% note warning %}

  {% include [base-zonal-pricing](../../_includes/managed-kubernetes/base-zonal-pricing.md) %}

  {% endnote %}

* _Высокодоступный_ — содержит три хоста мастера, которые вы можете разместить следующим образом:
  * В одной зоне доступности и одной подсети. Такой мастер подойдет, если вы хотите обеспечить высокую доступность кластера и уменьшить сетевую задержку внутри него.
  * В трех разных зонах доступности. Такой мастер обеспечивает наибольшую отказоустойчивость: при недоступности одной зоны мастер остается работоспособным.

  Внутренний IP-адрес высокодоступного мастера доступен только в пределах одной облачной сети {{ vpc-full-name }}.

  Прежнее название — _региональный_.

  {% note warning %}

  {% include [ha-regional-pricing](../../_includes/managed-kubernetes/ha-regional-pricing.md) %}

  {% endnote %}

Подробнее о настройках мастера см. на странице [{#T}](../operations/kubernetes-cluster/kubernetes-cluster-create.md).

### Вычислительные ресурсы мастера {#master-resources}

{% include [master-default-config](../../_includes/managed-kubernetes/master-default-config.md) %}

При [создании](../operations/kubernetes-cluster/kubernetes-cluster-create.md) и [изменении](../operations/kubernetes-cluster/kubernetes-cluster-update.md#manage-resources) кластера вы можете выбрать конфигурацию мастера, подходящую под ваши задачи.

{% include [master-config-preview-note](../../_includes/managed-kubernetes/master-config-preview-note.md) %}

На стадии Preview выбор конфигурации мастера доступен в [консоли управления]({{ link-console-main }}).

Доступны следующие конфигурации мастера на платформе Intel Cascade Lake с гарантированной долей vCPU 100%:

* **Standard** — стандартные хосты с соотношением объема RAM к vCPU, равным 4 к 1: {#master-standard}

  Количество vCPU | Объем RAM
  --- | ---
  2 | 8
  4 | 16
  8 | 32
  16 | 64
  32 | 128
  64 | 256
  80 | 320

* **CPU optimized** — хосты с уменьшенным соотношением RAM к vCPU, равным 2 к 1: {#master-cpu-optimized}

  Количество vCPU | Объем RAM
  --- | ---
  4 | 8
  8 | 16
  16 | 32
  32 | 64

* **Memory optimized** — хосты с увеличенным соотношением RAM к vCPU, равным 8 к 1: {#master-memory-optimized}

  Количество vCPU | Объем RAM
  --- | ---
  2 | 16
  4 | 32
  8 | 64
  16 | 128
  32 | 256

Изменение конфигурации мастера не требует остановки кластера {{ managed-k8s-name }}.

Также доступна возможность запретить изменение конфигурации мастера.

## Группа узлов {#node-group}

_Группа узлов_ — группа ВМ с одинаковой конфигурацией в кластере {{ k8s }}, на которых запускаются пользовательские контейнеры.

{% include [node-vm-explained-short](../../_includes/managed-kubernetes/node-vm-explained-short.md) %}

{% include [node-vm-manipulation-warning](../../_includes/managed-kubernetes/node-vm-manipulation-warning.md) %}

### Конфигурация {#config}


При создании группы узлов вы можете сконфигурировать следующие параметры ВМ:
* Тип ВМ.
* Тип и количество ядер (vCPU).
* Объем памяти (RAM) и диска.
* Параметры ядра.
  * _Безопасные_ (safe) параметры ядра изолированы между подами.
  * _Небезопасные_ (unsafe) параметры влияют на работу не только подов, но и узла в целом. В {{ managed-k8s-name }} нельзя изменять небезопасные параметры ядра, имена которых не были явно указаны при [создании группы узлов](../operations/node-group/node-group-create.md).

  {% note info %}

  Указывайте только параметры ядра, которые принадлежат [пространствам имен](#namespace), например, `net.ipv4.ping_group_range`. Параметры, которые не принадлежат пространствам имен, например, `vm.max_map_count`, необходимо разрешать непосредственно в операционной системе или при помощи DaemonSet с контейнерами в привилегированном режиме после создания [группы узлов {{ managed-k8s-name }}](#node-group).

  {% endnote %}

  Подробнее о параметрах ядра см. в [документации {{ k8s }}](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/).

В одном кластере {{ k8s }} можно создавать группы с разными конфигурациями и размещать их в разных зонах доступности.

Для {{ managed-k8s-name }} в качестве среды запуска контейнеров доступна только платформа [containerd](https://containerd.io/).

### Подключение к узлам группы {#node-connect-ssh}

К узлам группы можно подключаться следующими способами:
* через SSH-клиент с помощью стандартной пары SSH-ключей, см. [{#T}](../operations/node-connect-ssh.md);
* через SSH-клиент и YC CLI с помощью {{ oslogin }}, см. [{#T}](../operations/node-connect-oslogin.md).

### Политики taints и tolerations {#taints-tolerations}

_Taints_ — это особые политики, которые присваиваются узлам в группе. С помощью taint-политик можно запретить некоторым подам выполняться на определенных узлах. Например, можно указать, что поды для рендеринга должны запускаться только на [узлах с GPU](node-group/node-group-gpu.md).

Преимущества использования taint-политик:
* политики сохраняются, когда узел перезапускается или заменяется новым;
* при добавлении узлов в группу политики назначаются этому узлу автоматически;
* политики автоматически назначаются новым узлам при [масштабировании группы узлов](autoscale.md).

Назначить taint-политику на группу узлов можно при [создании](../operations/node-group/node-group-create.md) или [изменении группы](../operations/node-group/node-group-update.md#assign-taints). Если назначить taint-политику на уже созданную группу узлов или снять политику с группы, эта группа пересоздается. Сначала удаляются все узлы в группе, затем в нее добавляются узлы с новой конфигурацией.

Каждая taint-политика состоит из трех частей:

```text
<ключ> = <значение>:<эффект>
```

Список доступных taint-эффектов:
* `NO_SCHEDULE` — запретить запуск новых подов на узлах группы (уже запущенные поды продолжат работу);
* `PREFER_NO_SCHEDULE` — избегать запуска подов на узлах группы, если для запуска этих подов есть свободные ресурсы в других группах;
* `NO_EXECUTE` — завершить работу подов на узлах этой группы, расселить их в другие группы, а запуск новых подов запретить.

_Tolerations_ — это исключения из taint-политик. С помощью tolerations можно разрешить определенным подам работать на узлах, даже если taint-политика группы узлов препятствует этому.

Tolerations бывают двух типов:

  * `Equal` — срабатывает, если в taint-политике и toleration совпадают ключ, значение и эффект. Используется по умолчанию.

  * `Exists` — срабатывает, если в taint-политике и toleration совпадают ключ и эффект. Значение ключа не учитывается.

  Например, если для узлов в группе настроена taint-политика `key1=value1:NoSchedule`, разместить поды на узле с помощью tolerations можно так:

  ```yaml
  apiVersion: v1
  kind: Pod
  ...
  spec:
    ...
    tolerations:
    - key: "key1"
      operator: "Equal"
      value: "value1"
      effect: "NoSchedule"
  ```
  Или так:

  ```yaml
  apiVersion: v1
  kind: Pod
  ...
  spec:
    ...
    tolerations:
    - key: "key1"
      operator: "Exists"
      effect: "NoSchedule"
  ```

{% note info %}

Для системных подов автоматически назначаются tolerations, позволяющие им работать на всех доступных узлах.

{% endnote %}

Подробнее о taint-политиках и исключениях см. в [документации {{ k8s }}](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).

### Метки узлов {#node-labels}

_Метки узлов_ — механизм группировки узлов в {{ managed-k8s-name }}. Существуют два вида меток узлов:

* [Облачные метки](../../resource-manager/concepts/labels.md) — используются для логического разделения и маркировки ресурсов. Например, с помощью облачных меток можно [следить за расходами](../../billing/operations/get-folder-report.md#format) на различные группы узлов. Обозначаются в CLI как `template-labels` и в {{ TF }} — как `labels`.

  Облачные метки узлов составляются по следующим правилам:

    {% include [cloud-labels-restrictions-nodes](../../_includes/managed-kubernetes/cloud-labels-restrictions-nodes.md) %}

  Об управлении облачными метками читайте в разделе [Изменение группы узлов](../operations/node-group/node-group-update.md#manage-label).

* [{{ k8s }}-метки]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/) — используются для группировки объектов {{ k8s }} и [распределения подов по узлам кластера](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes). Обозначаются в CLI как `node-labels` и в {{ TF }} — как `node_labels`.

  При назначении {{ k8s }}-меток указывайте характеристики узлов, по которым вы будете группировать объекты. Примеры {{ k8s }}-меток см. в [документации {{ k8s }}]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/#причины-использования).

  Набор {{ k8s }}-меток `ключ: значение` может быть определен для каждого объекта. Для него все ключи должны быть уникальными.

  {% include [k8s-labels-restrictions-nodes](../../_includes/managed-kubernetes/k8s-labels-restrictions-nodes.md) %}

  Для управления {{ k8s }}-метками доступны [API {{ managed-k8s-name }}](../managed-kubernetes/api-ref/index.md) и [API {{ k8s }}]({{ k8s-docs }}/concepts/overview/kubernetes-api). Их особенности:

  * {{ k8s }}-метки, добавленные через API {{ k8s }}, могут быть потеряны, так как во время [обновления или изменения групп узлов](../operations/node-group/node-group-update.md) часть узлов пересоздается с другим именем, а часть старых — удаляется.
  * Если {{ k8s }}-метки созданы через API {{ managed-k8s-name }}, их не получится удалить через API {{ k8s }}. Иначе метки будут восстановлены после удаления.

  {% note warning %}

  Чтобы избежать потери меток, используйте API {{ managed-k8s-name }}.

  {% endnote %}

  О добавлении и удалении {{ k8s }}-меток читайте в разделе [{#T}](../operations/node-group/node-label-management.md). Если добавить или удалить метку, группа узлов не пересоздается.

Оба вида меток могут использоваться одновременно, например, при [создании группы узлов](../operations/node-group/node-group-create.md) через CLI или {{ TF }}.

## Под {#pod}

_Под_ — запрос на запуск одного или более контейнеров на одном узле группы. В рамках кластера {{ k8s }} каждый под имеет уникальный IP-адрес, чтобы приложения не конфликтовали при использовании портов.

Контейнеры описываются в поде через объект, написанный на языке JSON или YAML.

### Маскарадинг IP-адресов подов {#pod-ip-masquerade}

Если поду требуется доступ к ресурсам за пределами кластера, его IP-адрес будет заменен на IP-адрес узла, на котором работает под. Для этого в кластере используется механизм [маскарадинга IP-адресов](https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/).

По умолчанию, маскарадинг включен в направлении всего диапазона IP-адресов, кроме направлений CIDR подов и CIDR [адресов link-local](https://ru.wikipedia.org/wiki/Link-local_address).

Для реализации механизма маскарадинга на каждом узле кластера развернут под `ip-masq-agent`. Настройки этого пода хранятся в объекте ConfigMap с именем `ip-masq-agent`. Если необходимо отключить маскарадинг IP-адресов подов в определенном направлении, например, для доступа к ним через [VPN](../../glossary/vpn.md) или [{{ interconnect-full-name }}](../../interconnect/index.yaml), укажите в параметре `data.config.nonMasqueradeCIDRs` нужные диапазоны IP-адресов:

```yaml
...
data:
  config: |+
    nonMasqueradeCIDRs:
      - <CIDR_IP-адресов_в_направлении_которых_не_требуется_маскарадинг>
...
```

Чтобы посмотреть, как строятся правила для маскарадинга IP-адресов в `iptables` на конкретном узле, [подключитесь к узлу по SSH](../operations/node-connect-ssh.md) и выполните команду:

```bash
sudo iptables -t nat -L IP-MASQ -v -n
```

Подробнее см. на [странице ip-masq-agent на GitHub](https://github.com/kubernetes-sigs/ip-masq-agent).

## Сервис {#service}

[_Сервис_](service.md) — абстракция, которая обеспечивает функции сетевой балансировки нагрузки. Правила подачи трафика настраиваются для группы подов, объединенных набором меток.

По умолчанию сервис доступен только внутри конкретного кластера {{ k8s }}, но может быть общедоступным и получать [запросы извне](../operations/create-load-balancer.md#lb-create) кластера {{ k8s }}.

## Пространство имен {#namespace}

_Пространство имен_ — абстракция, которая логически изолирует ресурсы кластера {{ k8s }} и распределяет [квоты]({{ link-console-quotas }}) на них. Это полезно для разделения ресурсов разных команд и проектов в одном кластере {{ k8s }}.

## Сервисные аккаунты {#service-accounts}

В кластерах {{ managed-k8s-name }} используется два типа сервисных аккаунтов:
* **Облачные сервисные аккаунты**

  Эти аккаунты существуют на уровне отдельного каталога в облаке и могут использоваться как {{ managed-k8s-name }}, так и другими сервисами.

  Подробнее см. в разделе [{#T}](../security/index.md) и [{#T}](../../iam/concepts/users/service-accounts.md).
* **Сервисные аккаунты {{ k8s }}**

  Эти аккаунты существуют и действуют только на уровне отдельного кластера {{ managed-k8s-name }}. Они применяются {{ k8s }} для:
  * Аутентификации запросов к API кластера от приложений, развернутых в кластере.
  * Настройки прав доступа для этих приложений.

  Набор сервисных аккаунтов {{ k8s }} автоматически создается в пространстве имен `kube-system` при развертывании кластера {{ managed-k8s-name }}.

  Для аутентификации внутри кластера {{ k8s }}, к которому относится сервисный аккаунт, создайте токен этого аккаунта вручную.

  Подробнее см. в [{#T}](../operations/connect/create-static-conf.md), а также в [документации {{ k8s }}](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/).

{% note warning %}

Не путайте [облачные сервисные аккаунты](../security/index.md#sa-annotation) и сервисные аккаунты {{ k8s }}.

В документации сервиса под _сервисным аккаунтом_ понимается облачный сервисный аккаунт, если не указано иное.

{% endnote %}

## Статистика кластера {{ managed-k8s-name }} {#metrics}

{% include [metrics-resources-list](../../_includes/managed-kubernetes/metrics-resources-list.md) %}

{% include [metrics-k8s-tools](../../_includes/managed-kubernetes/metrics-k8s-tools.md) %}

Описание метрик приводится в разделе [{#T}](../../managed-kubernetes/metrics.md).

## Примеры использования {#examples}

* [{#T}](../tutorials/k8s-cluster-with-no-internet.md)
* [{#T}](../tutorials/kubernetes-backup.md)
* [{#T}](../tutorials/prometheus-grafana-monitoring.md)
* [{#T}](../tutorials/metrics-server.md)
* [{#T}](../tutorials/driverless-gpu.md)
