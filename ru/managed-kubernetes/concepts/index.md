---
title: "Взаимосвязь ресурсов сервиса {{ k8s }}"
description: "Основная сущность, которой оперирует сервис, — кластер {{ k8s }}. Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя."
---

# Взаимосвязь ресурсов в {{ managed-k8s-name }}

[{{ k8s }}](https://kubernetes.io/ru/) — система для управления контейнерными приложениями. {{ k8s }} предоставляет механизмы взаимодействия с кластером, с помощью которых осуществляется автоматизации развертывания, масштабирования и управления приложениями в контейнерах.

Основная сущность, которой оперирует сервис, — _кластер {{ k8s }}_.

## Кластер {{ k8s }} {#kubernetes-cluster}

Кластер {{ k8s }} состоит из мастера и одной или нескольких групп узлов. Мастер отвечает за управление кластером {{ k8s }}. На узлах запускаются контейнеризованные приложения пользователя.

Сервис полностью управляет мастером, а также следит за состоянием и работоспособностью группы узлов. Пользователь может управлять узлами напрямую, а также настраивать кластер {{ k8s }} с помощью консоли управления {{ yandex-cloud }}, CLI и API {{ managed-k8s-name }}.

{% note warning %}

Группам узлов {{ k8s }} требуется доступ в интернет для скачивания образов и компонентов.

Предоставить доступ в интернет можно следующими способами:
* Назначить каждому узлу в группе [публичный IP адрес](../../vpc/concepts/address.md#public-addresses).
* [Настроить виртуальную машину в качестве NAT-инстанса](../../tutorials/routing/nat-instance.md).
* [Включить функцию NAT в интернет](../../vpc/operations/enable-nat.md).

{% endnote %}

При работе с кластером {{ k8s }} на инфраструктуре {{ yandex-cloud }} задействуются следующие ресурсы:

Ресурс | Количество | Комментарий
--- | --- | ---
Подсеть | 2 | {{ k8s }} резервирует диапазоны IP-адресов, которые будут использоваться для подов и сервисов.
Публичный IP | N | В количество N входит:<br>* **Один** публичный IP для NAT-инстанса.<br>* Публичный IP **каждому** узлу в группе, если вы используете технологию one-to-one NAT.

## Мастер {#master}

_Мастер_ — компонент, который управляет кластером {{ k8s }}.

Мастер запускает управляющие процессы {{ k8s }}, которые включают сервер {{ k8s }} API, планировщик и контроллеры основных ресурсов. Жизненный цикл мастера управляется сервисом при создании или удалении кластера {{ k8s }}. Мастер отвечает за глобальные решения, которые выполняются на всех узлах кластера {{ k8s }}. Они включают в себя планирование рабочих нагрузок, таких как контейнерные приложения, управление жизненным циклом рабочих нагрузок и масштабированием.


Мастер бывает двух типов, которые отличаются расположением в [зонах доступности](../../overview/concepts/geo-scope.md):
* _Зональный_ — создается в подсети в одной зоне доступности.
* _Региональный_ — создается распределенно в трех подсетях в каждой зоне доступности. При недоступности одной зоны региональный мастер остается работоспособным.

  {% note warning %}

  Внутренний IP-адрес регионального мастера доступен только в пределах одной облачной сети {{ vpc-full-name }}.

  {% endnote %}



## Группа узлов {#node-group}

_Группа узлов_ — группа ВМ с одинаковой конфигурацией в кластере {{ k8s }}, на которых запускаются пользовательские контейнеры.

### Конфигурация {#config}

При создании группы узлов вы можете сконфигурировать следующие параметры ВМ:
* Тип ВМ.
* Тип и количество ядер (vCPU).
* Объем памяти (RAM) и диска.
* Параметры ядра.
  * _Безопасные_ (safe) параметры ядра изолированы между подами.
  * _Небезопасные_ (unsafe) параметры влияют на работу не только подов, но и узла в целом. В {{ managed-k8s-name }} нельзя изменять небезопасные параметры ядра, имена которых не были явно указаны при [создании группы узлов](../operations/node-group/node-group-create.md).

  Подробнее о параметрах ядра см. в [документации {{ k8s }}](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/).


В одном кластере {{ k8s }} можно создавать группы с разными конфигурациями и размещать их в разных зонах доступности.


Для {{ managed-k8s-name }} доступны следующие среды запуска контейнеров:
* [Платформа `Docker`](https://www.docker.com/) — выбирается по умолчанию при создании группы узлов.
* [Платформа `containerd`](https://containerd.io/) — может быть выбрана при создании или изменении группы узлов с {{ k8s }} версии 1.19 и выше.

### Подключение к узлам группы {#node-connect-ssh}

К узлам группы можно подключаться по SSH. О том, как это сделать, читайте в разделе [Подключиться к узлу по SSH](../operations/node-connect-ssh.md).

### Политики taints и tolerations {#taints-tolerations}

_Taints_ — это особые политики, которые присваиваются узлам в группе. С помощью taint-политик можно запретить некоторым подам выполняться на определенных узлах. Например, можно указать, что поды для рендеринга должны запускаться только на [узлах с GPU](node-group/node-group-gpu.md).

Преимущества использования taint-политик:
* политики сохраняются, когда узел перезапускается или заменяется новым;
* при добавлении узлов в группу политики назначаются этому узлу автоматически;
* политики автоматически назначаются новым узлам при [масштабировании группы узлов](autoscale.md).

Назначить taint-политику для группы узлов можно только при ее [создании](../operations/node-group/node-group-create.md).

{% note warning %}

Не путайте [{{ k8s }}-метки узлов](#node-labels) (`node_labels`), которыми управляет {{ managed-k8s-name }}, и taint-политики.

{% endnote %}

Каждая taint-политика состоит из трех частей:

```text
<ключ> = <значение>:<эффект>
```

Список доступных taint-эффектов:
* `NO_SCHEDULE` — запретить запуск новых подов на узлах группы (уже запущенные поды продолжат работу);
* `PREFER_NO_SCHEDULE` — избегать запуска подов на узлах группы, если для запуска этих подов есть свободные ресурсы в других группах;
* `NO_EXECUTE` — завершить работу подов на узлах этой группы, расселить их в другие группы, а запуск новых подов запретить.

_Tolerations_ — это исключения из taint-политик. С помощью tolerations можно разрешить определенным подам работать на узлах, даже если taint-политика группы узлов препятствует этому.

Например, если для узлов в группе настроена taint-политика `key1=value1:NoSchedule`, разместить поды на таком узле можно с помощью tolerations:

```yaml
apiVersion: v1
kind: Pod
...
spec:
  ...
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
```

{% note info %}

Для системных подов автоматически назначаются tolerations, позволяющие им работать на всех доступных узлах.

{% endnote %}

Подробнее о taint-политиках и исключениях см. в [документации {{ k8s }}](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).

## Под {#pod}

_Под_ — запрос на запуск одного или более контейнеров на одном узле группы. В рамках кластера {{ k8s }} каждый под имеет уникальный IP-адрес, чтобы приложения не конфликтовали при использовании портов.

Контейнеры описываются в поде через объект, написанный на языке JSON или YAML.

### Маскарадинг IP-адресов подов {#pod-ip-masquerade}

Если поду требуется доступ к ресурсам за пределами кластера, его IP-адрес будет заменен на IP-адрес узла, на котором работает под. Для этого в кластере используется механизм [маскарадинга IP-адресов](https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/).

По умолчанию, маскарадинг включен для всего диапазона IP-адресов подов.

Для реализации механизма маскарадинга на каждом узле кластера развернут под `ip-masq-agent`. Настройки этого пода хранятся в объекте ConfigMap с именем `ip-masq-agent`. Если необходимо отключить маскарадинг IP-адресов подов, например, для доступа к ним через VPN или [{{ interconnect-full-name }}](../../interconnect/concepts/index.md), укажите в параметре `data.config.nonMasqueradeCIDRs` нужные диапазоны IP-адресов:

```yaml
...
data:
  config: |+
    nonMasqueradeCIDRs:
      - <CIDR IP-адресов подов, для которых не требуется маскирование>
...
```

## Сервис {#service}

[_Сервис_](service.md) — абстракция, которая обеспечивает функции сетевой балансировки нагрузки. Правила подачи трафика настраиваются для группы подов, объединенных набором меток.

По умолчанию сервис доступен только внутри конкретного кластера {{ k8s }}, но может быть общедоступным и получать [запросы извне](../operations/create-load-balancer.md#lb-create) кластера {{ k8s }}.

## Пространство имен {#namespace}

_Пространство имен_ — абстракция, которая логически изолирует ресурсы кластера {{ k8s }} и распределяет [квоты]({{ link-console-quotas }}) на них. Это полезно для разделения ресурсов разных команд и проектов в одном кластере {{ k8s }}.

### Сервисные аккаунты {#service-accounts}

В кластерах {{ managed-k8s-name }} используется два типа сервисных аккаунтов:
* **Облачные сервисные аккаунты**

  Эти аккаунты существуют на уровне отдельного каталога в облаке и могут использоваться как {{ managed-k8s-name }}, так и другими сервисами.

  Подробнее см. в разделе [{#T}](../security/index.md) и [{#T}](../../iam/concepts/users/service-accounts.md).
* **Сервисные аккаунты {{ k8s }}**

  Эти аккаунты существуют и действуют только на уровне отдельного кластера {{ managed-k8s-name }}. Они применяются {{ k8s }} для:
  * Аутентификации запросов к API кластера от приложений, развернутых в кластере.
  * Настройки прав доступа для этих приложений.

  Набор сервисных аккаунтов {{ k8s }} автоматически создается в пространстве имен `kube-system` при развертывании кластера {{ managed-k8s-name }}.

  {{ k8s }} создает токен для каждого такого аккаунта. Этот токен используется для аутентификации внутри кластера {{ k8s }}, к которому относится аккаунт.

  Подробнее см. в [документации {{ k8s }}](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/).

{% note warning %}

Не путайте [облачные сервисные аккаунты](../security/index.md#sa-annotation) и сервисные аккаунты {{ k8s }}.

В документации сервиса под _сервисным аккаунтом_ понимается облачный сервисный аккаунт, если не указано иное.

{% endnote %}

### Метки узлов {#node-labels}

_Метки узлов_, `node_labels` — механизм группировки узлов в {{ k8s }}. С помощью меток узлов можно управлять распределением подов по узлам кластера. Подробнее читайте в [документации {{ k8s }}](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes).

{% note warning %}

Не путайте [облачные метки группы узлов](../../overview/concepts/services.md#labels) (`labels`) и [{{ k8s }}-метки узлов]({{ k8s-docs }}/concepts/overview/working-with-objects/labels/) (`node_labels`), которыми управляет {{ managed-k8s-name }}.

Мы рекомендуем управлять всеми метками узлов через [API {{ managed-k8s-name }}](../api-ref/NodeGroup/index.md), поскольку во время [обновления/изменения групп узлов](../operations/node-group/node-group-update.md), по умолчанию, часть узлов пересоздается с другим именем, а часть старых удаляется. Поэтому метки, добавленные через [{{ k8s }} API]({{ k8s-docs }}/concepts/overview/kubernetes-api) могут быть потеряны. И наоборот, удаление через {{ k8s }} API меток, созданных через API {{ managed-k8s-name }}, не имеет эффекта, — они будут восстановлены.

{% endnote %}

Метки узлов можно задать только при создании группы узлов. Для каждого объекта может быть определен набор меток узлов `ключ:значение`. Каждый ключ должен быть уникальным для данного объекта.

Ключи меток узлов могут состоять из двух частей: необязательного префикса и имени, которые разделены знаком `/`.

Префикс необязательная часть ключа. Требования к префиксу:
* Должен быть поддоменом DNS: серия DNS-меток, разделенных точками `.`.
* Длина — до 253 символов.
* За последним символом — `/`.

Имя обязательная часть ключа. Требования к имени:
* Длина — до 63 символов.
* Может содержать строчные буквы латинского алфавита, цифры, дефисы, нижние подчеркивания и точки.
* Первый и последние символы — буква или цифра.

Как управлять метками узлов, читайте в разделе [{#T}](../operations/node-group/node-label-management.md).