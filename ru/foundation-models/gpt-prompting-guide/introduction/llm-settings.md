# Параметры LLM

Работа с LLM обычно происходит через API. При создании промтов может быть полезно настроить и протестировать несколько параметров, чтобы найти наиболее подходящие комбинации. Настройка этих параметров важна для улучшения стабильности и качества ответов. Вам потребуется провести несколько экспериментов, чтобы определить правильные настройки для решения ваших задач. Ниже приведены общие настройки, с которыми вы столкнетесь при использовании различных провайдеров LLM.

## Температура {#temperature}

Температура (параметр `temperature`) определяет степень случайности в ответах модели. Чем ниже температура, тем более детерминированными будут результаты, так как всегда выбирается наиболее вероятный следующий [токен](../../concepts/yandexgpt/tokens.md). При низкой температуре вы с большой вероятностью будете получать *одинаковые* ответы на один и тот же вопрос. Увеличение температуры может привести к большей случайности, что способствует более разнообразным или креативным ответам. Также и повышается вероятность получить *разные* ответы на один и тот же вопрос. 

На практике это означает, что для задач, требующих фактических и сжатых ответов, таких как вопросы и ответы на основе фактов, лучше использовать низкое значение температуры: 0 или 0.1. Для генерации стихов или других творческих задач может быть полезно увеличить значение температуры: 0.5, 0.9 или 1.

## Максимальная длина {#max-tokens}

Вы можете управлять количеством токенов, которые генерирует модель, регулируя максимальную длину ответа (параметр `max_tokens`). Указание максимальной длины помогает предотвратить длинные или нерелевантные ответы и контролировать затраты. Модель учитывает этот параметр и старается генерировать текст, не превышающий максимальной длины.

{% note info %}

Обратите внимание, что ответы моделей и результаты ваших экспериментов могут варьироваться в зависимости от версии используемой LLM.

{% endnote %}