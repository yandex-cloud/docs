---
title: "Репликация в {{ mch-full-name }}"
description: "Из статьи вы узнаете, как работает репликация хостов кластера в {{ mch-full-name }}." 
---

# Репликация в {{ mch-name }}

{{ mch-name }} позволяет использовать для организации репликации и распределения запросов один из следующих механизмов:

* [{#T}](#ck).
* [{#T}](#zk) (по умолчанию).

Это позволяет использовать [реплицируемые таблицы](#replicated-tables) в кластере с несколькими хостами в [шарде](./sharding.md). При этом управление репликацией осуществляется автоматически.

## {{ CK }} {#ck}

{% include [ClickHouse Keeper preview note](../../_includes/mdb/mch/note-ck-preview.md) %}

{{ CK }} — сервис для репликации данных и выполнения распределенных DDL-запросов, реализующий совместимый с {{ ZK }} клиент-серверный протокол. В отличие от {{ ZK }}, {{ CK }} не требует для своей работы отдельных хостов, а выполняется на хостах {{ CH }}. Включить поддержку {{ CK }} можно только при [создании кластера](../operations/cluster-create.md).

Использование {{ CK }} накладывает следующие ограничения:

* Можно создавать кластеры только из трех или более хостов.
* Поддержку {{ CK }} нельзя включить или выключить после создания кластера.
* Кластеры, использующие хосты {{ ZK }}, нельзя перевести на использование {{ CK }}.
* [Перенести хост](../operations/host-migration.md) c {{ CK }} в другую зону доступности можно только через обращение в [службу поддержки]({{ link-console-support }}).

Подробнее о {{ CK }} см. в [документации {{ CH }}]({{ ch.docs }}/operations/clickhouse-keeper/).

## {{ ZK }} {#zk}

Если кластер был создан без поддержки {{ CK }}, перед добавлением в шард из одного хоста новых хостов необходимо [включить отказоустойчивость кластера](../operations/zk-hosts.md#add-zk), если она еще не включена. При этом в кластер будет добавлено 3 хоста {{ ZK }} — минимально необходимое количество для управления репликацией и отказоустойчивостью.

Включить отказоустойчивость и настроить хосты {{ ZK }} можно [после создания кластера](../operations/zk-hosts.md#add-zk) из одного хоста.


Также можно настроить хосты {{ ZK }} сразу при создании кластера из нескольких хостов. При этом:

* Если в [виртуальной сети](../../vpc/concepts/network.md) для кластера есть подсети в каждой из [зон доступности](../../overview/concepts/geo-scope.md), то в каждую подсеть автоматически будет добавлено по одному хосту {{ ZK }}, если не указать настройки этих хостов явно. При необходимости можно явно указать три хоста {{ ZK }} и их настройки при создании кластера.
* Если в виртуальной сети для кластера есть подсети только в некоторых зонах доступности, то нужно явно указать три хоста {{ ZK }} и их настройки при создании кластера.

* Если вы не указали подсети для этих хостов, {{ mch-short-name }} автоматически распределит их по подсетям той сети, к которой подключен кластер {{ CH }}.


Минимальное количество ядер для одного хоста {{ ZK }} зависит от суммарного количества ядер хостов {{ CH }}:

| Суммарное количество ядер хостов {{ CH }} | Минимальное количество ядер для одного хоста {{ ZK }} |
|-------------------------------------------|-------------------------------------------------------|
| Менее 48                                  | 2                                                     |
| 48 и более                                | 4                                                     |

Класс хостов {{ ZK }} можно изменить при настройке отказоустойчивости или при [изменении настроек кластера](../operations/update.md#change-resource-preset). Изменить настройки {{ ZK }} или подключиться к его хостам нельзя.

{% note warning %}

Хосты {{ ZK }}, если они есть, учитываются при расчете [потребления ресурсов]({{ link-console-quotas }}) и стоимости кластера.

{% endnote %}

## Реплицируемые таблицы {#replicated-tables}

{{ CH }} поддерживает автоматическую репликацию только для таблиц на [движке семейства ReplicatedMergeTree]({{ ch.docs }}/engines/table-engines/mergetree-family/replication/). Чтобы обеспечить репликацию, вы можете создать такие таблицы на каждом хосте по отдельности или использовать распределенный DDL-запрос.

{% note warning %}

Рекомендуется создавать реплицируемые таблицы на всех хостах кластера, иначе может произойти потеря данных после восстановления кластера из [резервной копии](backup.md) или [миграции хостов кластера](../operations/host-migration.md) в другую зону доступности.

{% endnote %}

Чтобы создать таблицу `ReplicatedMergeTree` на определенном хосте {{ CH }}, отправьте запрос следующего вида:

```sql
CREATE TABLE db_01.table_01 (
    log_date date,
    user_name String) ENGINE = ReplicatedMergeTree ('/table_01', '{replica}'
)
PARTITION BY log_date
ORDER BY
    (log_date, user_name);
```

Здесь:

* `db_01` — имя базы данных.
* `table_01` — имя таблицы.
* `/table_01` — путь к таблице в {{ ZK }} или {{ CK }}, обязательно должен начинаться с прямого слэша `/`.
* `{replica}` — макроподстановка идентификатора хоста.

Чтобы создать реплицируемые таблицы на всех хостах кластера, отправьте [распределенный DDL-запрос]({{ ch.docs }}/sql-reference/distributed-ddl/):

```sql
CREATE TABLE db_01.table_01 ON CLUSTER '{cluster}' (
    log_date date,
    user_name String) ENGINE = ReplicatedMergeTree ('/table_01', '{replica}'
)
PARTITION BY log_date
ORDER BY
    (log_date, user_name);
```

Аргумент `'{cluster}'` автоматически разрешится в идентификатор кластера {{ CH }}.

Об организации взаимодействия реплицированных и распределенных таблиц в кластере {{ CH }} см. в разделе [Шардирование](sharding.md).

{% include [clickhouse-disclaimer](../../_includes/clickhouse-disclaimer.md) %}
