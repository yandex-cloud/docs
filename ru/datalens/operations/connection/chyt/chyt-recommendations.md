
# Рекомендации по созданию подключения к CHYT


## Разделение датасетов по подключениям {#separation-by-connections}

* Подключение в CHYT содержит данные о клике и токене пользователя, от имени которого совершается запрос. Для команд и подразделений создавайте подключения на каждую клику в кластере с токеном робота.
* Если в подключении использован личный токен, ограничьте права на это подключение правом **Исполнение**. Тогда другие пользователи смогут смотреть чарты, созданные поверх этого подключения, но не смогут создавать новые датасеты.

## Создание быстрых дашбордов {#creating-quick-dashboards}

* Используйте максимально возможую предагрегацию данных. Например, если данные в миллисекундах, а аналитику нужно строить по дням, то нужно предагрегировать данные до дня.
* Уменьшите количество вычислений налету. Например, дату можно считать по полю `datetime`, но фильтрация по этому полю будет медленней.
* Старайтесь не использовать оператор JOIN. Этот оператор замедляет запросы.
* Перенесите обработку на SSD-диск, если [рекомендации от CHYT](#Recommendations-from-chyt) не помогли. На SSD-диске таблицы обрабатываются быстрее.

## Параметры кеширования и количество графиков на странице дашборда {#caching-and-graphics-options}

* При открытии дашборда отправляются отдельные запросы по всем селекторам и чартам. Несколько запросов отправляются последовательно. При этом отправляется текущая страница и еще одна страница (веб-страница). Все чарты не загружаются в конце дашборда. Поэтому для ускорения загрузки оптимизируйте данные и ресурсы клики.
* Не рекомендуется заводить разные клики.
* По умолчанию время жизни кеша 5 минут. Если данные обновляются реже, например, один раз в день, увеличьте время жизни до одного часа.

## Рекомендации от CHYT {#Recommendations-from-chyt}

Чаще всего скорость обработки запросов снижается при чтении (не только при непосредственном чтении с диска, но и при разжатии данных, перекладывании из формата хранения {{ ytsaurus-name }} в CH и т. д.). В этом случае не всегда помогает увеличение количества инстансов, поэтому рекомендуем изменить формат хранения данных:
* Чтобы при обработке не читались лишние столбцы, таблицы должны быть c аттрибутом `optimize_for=scan`.
* Если при чтении данных используются различные фильтры, то отсортируйте таблицы. Если фильтрация происходит по ключу сортировки, то ненужные чанки будут отфильтрованы до чтения данных из них. 

{% note info %}

В некоторых случаях, если отсортировать таблицу, эффективной фильтрации не будет. Колонка строковая и конвертация `DateTime` <-> `String` не является монотонной и однозначной. Преобразование `Int` <-> `DateTime` монотонное, но со строковым представлением применять такую оптимизацию в общем случае нельзя. Например, `2020-01-01 00:00:00` и `2020-01-01T00:00:00` - корректное представление одного и того же момента времени в {{ CH }}, но при сортировке со строковым представлением между ними может появиться значение `2020-01-01 00:00:01`, поэтому преобразование `String` -> `DateTime` не является монотонным и использовать эту оптимизацию нельзя.

{% endnote %}

* Не используйте стирающий код. Он предназначен для "холодных" данных, дает повышенную нагрузку на репликацию, и чтение таких таблиц неэффективно. Запросы в таблицу без `erasure_codec` гораздо быстрее, чем для тех же таблиц с `erasure_codec`. 

{% note warning %}

Простая смена аттрибута `erasure_codec` на `optimize_for` не изменит формат данных. Чтобы форсировать изменение, запустите операцию `merge` с опцией `force_transform=%true`.

{% endnote %}

* Старайтесь не использовать динамические таблицы. Любое чтение из динамической таблицы требует чтения версий и всего ключа. При этом ридеры менее параллельные, увеличиваются накладные расходы на конвертацию в формат {{ CH }} и необходимо мержить строки с одинаковыми ключами. В итоге чтение из динамических таблиц гораздо медленнее. Точная цифра зависит от таблицы и паттерна работы с ней, но разница может доходить до 10 раз.
* Избегайте выполнения неоправданно тяжелых запросов. Например, сортировка 100ГБ, SELECT DISTINCT или GROUP BY по колонке, которая имеет миллионы различных значений, или использование оператора JOIN для больших таблиц. Такие запросы всегда будут обрабатываться медленно.
* Ускоряйте чтение с помощью повышения `replication_factor` на таблице.

При построении датасета по диапазону таблиц {{ datalens-short-name }} использует CHYT-функцию `concatYtTablesRange`. Функция выводит наиболее общую схему всех таблиц. В результирующий набор попадают столбцы, которые присутствуют во всех таблицах и имеют в них одинаковый тип. Чтобы вывести данные по всем столбцам всех таблиц диапазона, добавьте недостающие столбцы в более старые таблицы с помощью `alter-table`.

## Полезные ссылки {#links}

* [Производительность CHYT](https://ytsaurus.tech/docs/ru/user-guide/data-processing/chyt/reference/performance)
* [Визуализация данных из CHYT](../../../tutorials/data-from-ch-over-yt.md)
* [Управление доступом к {{ datalens-short-name }}](../../../security/index.md)
