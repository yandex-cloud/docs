# Выделенные инстансы

{% include [public-preview](../../../_includes/preview-pp.md) %}

{{ foundation-models-name }} позволяет развернуть некоторые модели на _выделенном инстансе_. В отличие от самостоятельного развертывания моделей на ВМ в сервисе {{ compute-full-name }}, вам не нужно настраивать окружение и подбирать оптимальные параметры ВМ — {{ foundation-models-name }} обеспечивает стабильный, надежный и эффективный инференс модели и следит за его работой в автоматическом режиме. 

Выделенные инстансы имеют ряд преимуществ:

* Гарантируемые параметры производительности, на которые не влияет трафик других пользователей.
* Отсутствие дополнительных квот на отправление запросов и параллельные генерации, ограничения зависят только от выбранной конфигурации инстанса.
* Оптимизированный инференс модели, чтобы обеспечить эффективное использование оборудования.

Выделенные инстансы будут полезны, если вам необходимо обрабатывать большие объемы запросов без задержек. [Тарификация](../../pricing.md) выделенного инстанса не зависит от объема входящих и исходящих токенов: оплачиваться будет только время его работы. 

## Модели выделенного инстанса {#models}

Все развернутые модели доступны через API, совместимый с [{{ openai }}](../openai-compatibility.md), {{ ml-sdk-name }} и в {{ ai-playground }}. Чтобы развернуть выделенный инстанс, понадобится [роль](../../security/index.md) `ai.models.editor` или выше на каталог. Для обращения к модели достаточно роли `ai.languageModels.user`.

{% note warning %}

C 10 ноября модели **gpt-oss-20b** и **gpt-oss-120b** будут доступны только в [базовом инстансе](models.md).

{% endnote %}

#|
|| **Модель** | **Контекст** | **Лицензия** ||
|| **Qwen 2.5 VL 32B Instruct**
[Карточка модели](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct) | 32 768 | Лицензия [Apache 2.0]({{ license-apache }}) ||
|| **Qwen 2.5 7B Instruct** 
[Карточка модели](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) | 32 768 | [Лицензия Qwen](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE) ||
|| **Gemma 3 4B it**
[Карточка модели](https://huggingface.co/google/gemma-3-4b-it) | 131 072 | [Условия использования Gemma]({{ license-gemma }})  ||
|| **Gemma 3 12B it**
[Карточка модели](https://huggingface.co/google/gemma-3-12b-it) | 65 536 | [Условия использования Gemma]({{ license-gemma }}) ||
|| **gpt-oss-20b** 
[Карточка модели](https://huggingface.co/openai/gpt-oss-20b) | 32 768  | Лицензия [Apache 2.0]({{ license-apache }}) ||
|| **gpt-oss-120b** 
[Карточка модели](https://huggingface.co/openai/gpt-oss-120b) | 32 768  | Лицензия [Apache 2.0]({{ license-apache }})  ||
|| **T-pro-it-2.0-FP8** 
[Карточка модели](https://huggingface.co/t-tech/T-pro-it-2.0-FP8) | 32 768 | Лицензия [Apache 2.0]({{ license-apache }}) ||
|#

## Конфигурации выделенных инстансов {#config}

Каждая модель может быть доступна для развертывания на нескольких конфигурациях: **S**, **M** или **L**. Каждая конфигурация гарантирует определенные значения TTFT (_Time to first token_, время до первого токена), _Latency_ (задержка — время, затраченное на генерацию ответа) и TPS (_Tokens per second_, количество токенов в секунду) для запросов с разной длиной контекста.

Рисунок ниже показывает зависимость задержек и количества токенов, обрабатываемых моделью, от количества параллельных генераций (Concurrency на рисунке): до определенного момента чем больше генераций модель будет обрабатывать параллельно, тем дольше будет длиться генерация и тем больше токенов будет сгенерировано за секунду.

![instances](../../../_assets/ai-studio/instances-chart.svg)