---
editable: false
noIndex: true
sourcePath: en/_cli-ref-beta/cli-ref-beta/managed-kubernetes/cli-ref-beta/node-group/create.md
---

# yc beta managed-kubernetes node-group create

Creates a node group in the specified Kubernetes cluster.

#### Command Usage

Syntax: 

`yc beta managed-kubernetes node-group create <CLUSTER-ID>`

#### Flags

| Flag | Description |
|----|----|
|`-r`,`--request-file`|<b>`string`</b><br/>Path to a request file.|
|`--example-json`|Generates a JSON template of the request.<br/><br/>The template can be customized and used as input for the command.<br/><br/>Usage example:<br/>1. Generate template: yc beta compute instance create --example-json > request.json<br/>2. Edit the template: vim request.json<br/>3. Run with template: yc beta compute instance create -r request.json|
|`--example-yaml`|Generates a YAML template of the request.<br/><br/>The template can be customized and used as input for the command.<br/><br/>Usage example:<br/>1. Generate template: yc beta compute instance create --example-yaml > request.yaml<br/>2. Edit the template: vim request.yaml<br/>3. Run with template: yc beta compute instance create -r request.yaml|
|`--allocation-policy`|<b>`shorthand/json`</b><br/>Allocation policy of the node group by the zones and regions.<br/><br/>Example:|
|`--allocation-policy`|<b>`locations=[{subnet-id=value,`</b><br/>zone-id=value}]<br/><br/>Shorthand Syntax:<br/>{locations=[{subnet-id=str, zone-id=str},...]}<br/>Fields:<br/>locations  []struct  — List of locations where resources for the node group will be allocated.<br/>subnet-id  string            — ID of the subnet. If a network chosen for the Kubernetes cluster has only one subnet in the specified zone, subnet ID may be omitted.<br/>zone-id    string  required  — ID of the availability zone where the nodes may reside.<br/>|
|`--allowed-unsafe-sysctls`|<b>`strings`</b><br/>Support for unsafe sysctl parameters. For more details see [documentation](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/).|
|`--cluster-id`|<b>`string`</b><br/>ID of the Kubernetes cluster to create a node group in.<br/>To get the Kubernetes cluster ID, use a [ClusterService.List] request.|
|`--deploy-policy`|<b>`shorthand/json`</b><br/>Deploy policy according to which the updates are rolled out. If not specified,<br/>the default is used.<br/><br/>Example:|
|`--deploy-policy`|<b>`max-expansion=1,`</b><br/>max-unavailable=1<br/><br/>Shorthand Syntax:<br/>{max-expansion=int, max-unavailable=int}<br/>Fields:<br/>max-expansion    int  — The maximum number of instances that can be temporarily allocated above<br/>the group's target size during the update process.<br/>If [max_unavailable] is not specified or set to zero, [max_expansion] must<br/>be set to a non-zero value.<br/>max-unavailable  int  — The maximum number of running instances that can be taken offline (i.e.,<br/>stopped or deleted) at the same time during the update process.<br/>If [max_expansion] is not specified or set to zero, [max_unavailable] must<br/>be set to a non-zero value.<br/>|
|`--description`|<b>`string`</b><br/>Description of the node group.|
|`--labels`|<b>`stringToString`</b><br/>Resource labels as 'key:value' pairs.|
|`--maintenance-policy`|<b>`shorthand/json`</b><br/>Maintenance policy of the node group.<br/><br/>Example:|
|`--maintenance-policy`|<b>`auto-repair=true,`</b><br/>auto-upgrade=true, maintenance-window={policy={anytime={}}}<br/><br/>Shorthand Syntax:<br/>{auto-repair=bool, auto-upgrade=bool, maintenance-window={policy={anytime={} \| daily-maintenance-window={duration=duration, start-time=timeofday} \| weekly-maintenance-window={days-of-week=[{days=MONDAY\|TUESDAY\|WEDNESDAY\|THURSDAY\|FRIDAY\|SATURDAY\|SUNDAY,..., duration=duration, start-time=timeofday},...]}}}}<br/>Fields:<br/>auto-repair         bool    — If set to true, automatic repairs are enabled. Default value is false.<br/>auto-upgrade        bool    — If set to true, automatic updates are installed in the specified period of time with no interaction from the user.<br/>If set to false, automatic upgrades are disabled.<br/>maintenance-window  struct  — Maintenance window settings. Update will start at the specified time and last no more than the specified duration.<br/>The time is set in UTC.<br/>policy  oneof<anytime\|daily-maintenance-window\|weekly-maintenance-window>  — Oneof policy field<br/>anytime                    struct  — Updating the master at any time.<br/>daily-maintenance-window   struct  — Updating the master on any day during the specified time window.<br/>duration    duration             — Window duration.<br/>start-time  timeofday  required  — Window start time, in the UTC timezone.<br/>weekly-maintenance-window  struct  — Updating the master on selected days during the specified time window.<br/>days-of-week  []struct  — Days of the week and the maintenance window for these days when automatic updates are allowed.<br/>days        []int                — Days of the week when automatic updates are allowed.<br/>duration    duration             — Window duration.<br/>start-time  timeofday  required  — Window start time, in the UTC timezone.<br/>|
|`--name`|<b>`string`</b><br/>Name of the node group.<br/>The name must be unique within the folder.|
|`--node-labels`|<b>`stringToString`</b><br/>Labels that are assigned to the nodes of the node group at creation time.|
|`--node-taints`|<b>`shorthand/json`</b><br/>Taints that are applied to the nodes of the node group at creation time.<br/><br/>Example:|
|`--node-taints`|<b>`[{effect=NO_SCHEDULE,`</b><br/>key=value, value=value}]<br/><br/>Shorthand Syntax:<br/>[{effect=NO_SCHEDULE\|PREFER_NO_SCHEDULE\|NO_EXECUTE, key=str, value=str},...]<br/>Fields:<br/>effect  enum<NO_EXECUTE\|NO_SCHEDULE\|PREFER_NO_SCHEDULE>  — The effect of the taint on pods that do not tolerate the taint.<br/>key     string                                           — The taint key to be applied to a node.<br/>value   string                                           — The taint value corresponding to the taint key.<br/>|
|`--node-template`|<b>`shorthand/json`</b><br/>Node template for creating the node group.<br/><br/>Example:|
|`--node-template`|<b>`boot-disk-spec={disk-size=1,`</b><br/>disk-type-id=value}, container-network-settings={pod-mtu=1}, container-runtime-settings={type=DOCKER}, gpu-settings={gpu-cluster-id=value, gpu-environment=RUNC_DRIVERS_CUDA}, labels={key=value}, metadata={key=value}, name=value, network-interface-specs=[{primary-v4-address-spec={dns-record-specs=[{dns-zone-id=value, fqdn=value, ptr=true, ttl=1}], one-to-one-nat-spec={ip-version=IPV4}}, primary-v6-address-spec={dns-record-specs=[{dns-zone-id=value, fqdn=value, ptr=true, ttl=1}], one-to-one-nat-spec={ip-version=IPV4}}, security-group-ids=value, subnet-ids=value}], network-settings={type=STANDARD}, placement-policy={placement-group-id=value}, platform-id=value, resources-spec={core-fraction=1, cores=1, gpus=1, memory=1}, scheduling-policy={preemptible=true}, v4-address-spec={dns-record-specs=[{dns-zone-id=value, fqdn=value, ptr=true, ttl=1}], one-to-one-nat-spec={ip-version=IPV4}}<br/><br/>Shorthand Syntax:<br/>{boot-disk-spec={disk-size=int, disk-type-id=str}, container-network-settings={pod-mtu=int}, container-runtime-settings={type=DOCKER\|CONTAINERD}, gpu-settings={gpu-cluster-id=str, gpu-environment=RUNC_DRIVERS_CUDA\|RUNC}, labels={key=str, key=...}, metadata={key=str, key=...}, name=str, network-interface-specs=[{primary-v4-address-spec={dns-record-specs=[{dns-zone-id=str, fqdn=str, ptr=bool, ttl=int},...], one-to-one-nat-spec={ip-version=IPV4\|IPV6}}, primary-v6-address-spec={dns-record-specs=[{dns-zone-id=str, fqdn=str, ptr=bool, ttl=int},...], one-to-one-nat-spec={ip-version=IPV4\|IPV6}}, security-group-ids=str,..., subnet-ids=str,...},...], network-settings={type=STANDARD\|SOFTWARE_ACCELERATED}, placement-policy={placement-group-id=str}, platform-id=str, resources-spec={core-fraction=int, cores=int, gpus=int, memory=int}, scheduling-policy={preemptible=bool}, v4-address-spec={dns-record-specs=[{dns-zone-id=str, fqdn=str, ptr=bool, ttl=int},...], one-to-one-nat-spec={ip-version=IPV4\|IPV6}}}<br/>Fields:<br/>boot-disk-spec              struct              — Specification for the boot disk that will be attached to the node.<br/>disk-size     int     — Size of the disk, specified in bytes.<br/>disk-type-id  string  — ID of the disk type.<br/>container-network-settings  struct              —<br/>pod-mtu  int  —<br/>container-runtime-settings  struct              —<br/>type  enum<CONTAINERD\|DOCKER>  required  —<br/>gpu-settings                struct              — GPU settings<br/>gpu-cluster-id   string                        — GPU cluster id, that mk8s node will join.<br/>gpu-environment  enum<RUNC\|RUNC_DRIVERS_CUDA>  — GPU environment configured on node.<br/>labels                      map[string,string]  — these labels will be assigned to compute nodes (instances), created by the nodegroup<br/>metadata                    map[string,string]  — The metadata as 'key:value' pairs assigned to this instance template. Only SSH keys are supported as metadata.<br/><br/>For more information, see [Connecting to a node over SSH](https://yandex.cloud/ru/docs/managed-kubernetes/operations/node-connect-ssh).<br/>name                        string              — Name of the instance.<br/>In order to be unique it must contain at least on of instance unique placeholders:<br/>{instance.short_id}<br/>{instance.index}<br/>combination of {instance.zone_id} and {instance.index_in_zone}<br/>Example: my-instance-{instance.index}<br/>If not set, default is used: {instance_group.id}-{instance.short_id}<br/>It may also contain another placeholders, see metadata doc for full list.<br/>network-interface-specs     []struct            — New api, to specify network interfaces for the node group compute instances.<br/>Can not be used together with 'v4_address_spec'<br/>primary-v4-address-spec  struct    — Primary IPv4 address that is assigned to the instance for this network interface.<br/>dns-record-specs     []struct  — Internal DNS configuration.<br/>dns-zone-id  string            — DNS zone id (optional, if not set, private zone is used).<br/>fqdn         string  required  — FQDN (required).<br/>ptr          bool              — When set to true, also create PTR DNS record (optional).<br/>ttl          int               — DNS record ttl, values in 0-86400 (optional).<br/>one-to-one-nat-spec  struct    — One-to-one NAT configuration. Setting up one-to-one NAT ensures that public IP addresses are assigned to nodes, and therefore internet is accessible for all nodes of the node group. If the field is not set, NAT will not be set up.<br/>ip-version  enum<IPV4\|IPV6>  — IP version for the public IP address.<br/>primary-v6-address-spec  struct    — Primary IPv6 address that is assigned to the instance for this network interface.<br/>dns-record-specs     []struct  — Internal DNS configuration.<br/>dns-zone-id  string            — DNS zone id (optional, if not set, private zone is used).<br/>fqdn         string  required  — FQDN (required).<br/>ptr          bool              — When set to true, also create PTR DNS record (optional).<br/>ttl          int               — DNS record ttl, values in 0-86400 (optional).<br/>one-to-one-nat-spec  struct    — One-to-one NAT configuration. Setting up one-to-one NAT ensures that public IP addresses are assigned to nodes, and therefore internet is accessible for all nodes of the node group. If the field is not set, NAT will not be set up.<br/>ip-version  enum<IPV4\|IPV6>  — IP version for the public IP address.<br/>security-group-ids       []string  — IDs of security groups.<br/>subnet-ids               []string  — IDs of the subnets.<br/>network-settings            struct              — this parameter allows to specify type of network acceleration used on nodes (instances)<br/>type  enum<SOFTWARE_ACCELERATED\|STANDARD>  required  —<br/>placement-policy            struct              —<br/>placement-group-id  string  — Identifier of placement group<br/>platform-id                 string              — ID of the hardware platform configuration for the node.<br/>resources-spec              struct              — Computing resources of the node such as the amount of memory and number of cores.<br/>core-fraction  int  — Baseline level of CPU performance with the possibility to burst performance above that baseline level.<br/>This field sets baseline performance for each core.<br/>cores          int  — Number of cores available to the node.<br/>gpus           int  — Number of GPUs available to the node.<br/>memory         int  — Amount of memory available to the node, specified in bytes.<br/>scheduling-policy           struct              — Scheduling policy configuration.<br/>preemptible  bool  — True for preemptible compute instances. Default value is false. Preemptible compute instances are stopped at least once every 24 hours, and can be stopped at any time<br/>if their resources are needed by Compute.<br/>For more information, see [Preemptible Virtual Machines](https://yandex.cloud/ru/docs/compute/concepts/preemptible-vm).<br/>v4-address-spec             struct              — Specification for the create network interfaces for the node group compute instances.<br/>Deprecated, please use network_interface_specs.<br/>dns-record-specs     []struct  — Internal DNS configuration.<br/>dns-zone-id  string            — DNS zone id (optional, if not set, private zone is used).<br/>fqdn         string  required  — FQDN (required).<br/>ptr          bool              — When set to true, also create PTR DNS record (optional).<br/>ttl          int               — DNS record ttl, values in 0-86400 (optional).<br/>one-to-one-nat-spec  struct    — One-to-one NAT configuration. Setting up one-to-one NAT ensures that public IP addresses are assigned to nodes, and therefore internet is accessible for all nodes of the node group. If the field is not set, NAT will not be set up.<br/>ip-version  enum<IPV4\|IPV6>  — IP version for the public IP address.<br/>|
|`--scale-policy`|<b>`shorthand/json`</b><br/>Scale policy of the node group.<br/><br/>Example:|
|`--scale-policy`|<b>`scale-type={auto-scale={initial-size=1,`</b><br/>max-size=1, min-size=1}}<br/><br/>Shorthand Syntax:<br/>{scale-type={auto-scale={initial-size=int, max-size=int, min-size=int} \| fixed-scale={size=int}}}<br/>Fields:<br/>scale-type  oneof<auto-scale\|fixed-scale>  — Oneof scale-type field<br/>fixed-scale  struct  — Fixed scale policy of the node group.<br/>size  int  — Number of nodes in the node group.<br/>auto-scale   struct  — Auto scale policy of the node group.<br/>initial-size  int  — Initial number of nodes in the node group.<br/>max-size      int  — Maximum number of nodes in the node group.<br/>min-size      int  — Minimum number of nodes in the node group.<br/>|
|`--version`|<b>`string`</b><br/>Version of Kubernetes components that runs on the nodes.|
|`--async`|Display information about the operation in progress, without waiting for the operation to complete.|

#### Global Flags

| Flag | Description |
|----|----|
|`--profile`|<b>`string`</b><br/>Set the custom profile.|
|`--region`|<b>`string`</b><br/>Set the region.|
|`--debug`|Debug logging.|
|`--debug-grpc`|Debug gRPC logging. Very verbose, used for debugging connection problems.|
|`--no-user-output`|Disable printing user intended output to stderr.|
|`--pager`|<b>`string`</b><br/>Set the custom pager.|
|`--format`|<b>`string`</b><br/>Set the output format: text, yaml, json, table, json-rest.|
|`--retry`|<b>`int`</b><br/>Enable gRPC retries. By default, retries are enabled with maximum 5 attempts.<br/>Pass 0 to disable retries. Pass any negative value for infinite retries.<br/>Even infinite retries are capped with 2 minutes timeout.|
|`--timeout`|<b>`string`</b><br/>Set the timeout.|
|`--token`|<b>`string`</b><br/>Set the IAM token to use.|
|`--impersonate-service-account-id`|<b>`string`</b><br/>Set the ID of the service account to impersonate.|
|`--no-browser`|Disable opening browser for authentication.|
|`--query`|<b>`string`</b><br/>Query to select values from the response using jq syntax|
|`-h`,`--help`|Display help for the command.|
