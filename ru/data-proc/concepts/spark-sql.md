# Задания Spark

{{ dataproc-name }} поддерживает выполнение заданий [Spark](https://spark.apache.org/docs/latest/sql-programming-guide.html) в рамках Spark-_приложений_ (applications). За распределение ресурсов при выполнении заданий отвечает [Apache Hadoop YARN](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html).

## Управление приложениями {#applications}

В кластере может быть запущено несколько приложений одновременно. Запущенным приложением управляет специальная программа — _драйвер_ (driver). Подробнее о работе драйвера см. в подразделе [распределение ресурсов](#resource-management).

Приложение может находиться в режиме ожидания, либо выполнять _задания_ (jobs). По умолчанию задания в рамках одного приложения выполняются последовательно (_FIFO_). Этот метод не требует дополнительных настроек.

Чтобы включить параллельное выполнение заданий (_FAIR_), необходимо переключить режим работы планировщика, а также дополнительно настроить систему и конкретные задания. О том, как это сделать, читайте в [документации Apache Spark](https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application).

{% note tip %}

Чтобы реализовать параллельное выполнение заданий, не используя FAIR, запустите задания в разных Spark-приложениях в режиме FIFO.

{% endnote %}

## Управление операциями {#tasks}

Каждое задание Spark состоит из набора _операций_ (tasks), которые выполняются в рамках специальных программ — _исполнителей_ (executors). Каждый исполнитель запускается на одном хосте кластера и занимает определенное количество вычислительных ресурсов (CPU, RAM).

В зависимости от плана выполнения, операции могут выполняться последовательно или параллельно. Параллельно выполняемые операции группируются в _стадии_ (stages). Количество параллельно выполняемых операций зависит от потребностей запросов и от доступных ресурсов кластера.

При использовании стандартных настроек {{ dataproc-name }} вычислительные ресурсы на выполнение заданий Spark выделяются динамически.

## Распределение ресурсов {#resource-management}

Распределение ресурсов между драйвером и исполнителями зависит от [свойств компонента](./settings-list.md) Spark. Набор ключевых свойств и установленные для них по умолчанию значения зависят от режима размещения драйвера, который задается свойством `spark:spark.submit.deployMode`:

* `deployMode=client` — драйвер размещается на управляющем хосте кластера (`master`). Режим используется по умолчанию, если кластер отвечает требованиям для [легковесных кластеров](./index.md#light-weight-clusters).
* `deployMode=cluster` — драйвер размещается на одном из вычислительных хостов (`compute`) кластера. Режим используется по умолчанию, если кластер не отвечает требованиям для [легковесных кластеров](./index.md#light-weight-clusters).

В таблицах ниже перечислены ключевые свойства, отвечающие за распределение ресурсов при разных режимах размещения драйвера.

В таблицах приняты сокращения:

* `allCPU` — количество ядер хоста. Определяется классом хостов, который выбран на этапе создания подкластера.
* `nmMem` — объем оперативной памяти хоста, доступной для YARN NodeManager. Вычисляется по формуле:

    `общая память хоста` × `доля памяти для YARN NodeManager`.

    * Общая память хоста определяется классом хостов, который выбран на этапе создания подкластера.
    * Доля памяти, резервируемая для YARN NodeManager, задается свойством `dataproc:nodemanager_available_memory_ratio` и по умолчанию составляет `0.8`. Остальная память резервируется для вспомогательной нагрузки (отправка логов, метрик и т. п.).

Результаты арифметических операций в таблицах округляются:

* Для CPU — в меньшую сторону до целой части. Если результат деления оказывается меньше 1, то он округляется до 1.
* Для оперативной памяти — в меньшую сторону до значения, кратного 1 ГБ.

{% list tabs %}

- deployMode=client

    В этом режиме драйвер запускается на управляющем хосте кластера отдельно от YARN Application Master, и ему доступны все ресурсы управляющего хоста. Для работы YARN Application Master на вычислительных хостах резервируется лишь необходимый небольшой объем ресурсов.

    #|
    || **Свойство (сокращение)**                    | **Описание**                                                       | **Значение по умолчанию**        ||
    || `dataproc:spark_executors_per_vm` (`numCon`) | Максимальное количество контейнеров на одном вычислительном хосте  | `1`                              ||
    || `spark:spark.yarn.am.cores` (`yamCPU`)       | Количество ядер процессора, выделяемых для YARN Application Master | `1`                              ||
    || `spark:spark.yarn.am.memory` (`yamMem`)      | Объем памяти (МБ), выделяемый для YARN Application Master          | `1024`                           ||
    || `spark:spark.executor.cores` (`exCPU`)       | Количество ядер процессора, выделяемых для каждого исполнителя     | (`allCPU` − `yamCPU`) / `numCon` ||
    || `spark:spark.executor.memory` (`exMem`)      | Объем памяти (МБ), выделяемый на каждого исполнителя               | (`nmMem` − `yamMem`) / `numCon`  ||
    |#

    Поскольку значения `yamCPU` и `yamMem` вычитаются из общего количества CPU и RAM, соответственно, YARN Application Master занимает меньше ресурсов, чем стандартный контейнер, а доля ресурсов для исполнителей увеличивается.

- deployMode=cluster

    Этот режим предполагает, что на управляющем хосте кластера запущена требовательная к ресурсам программа, например, HDFS. Поэтому драйверы запускаются на вычислительных хостах в рамках YARN Application Master, и для них резервируется значительный объем ресурсов.

    #|
    || **Свойство (сокращение)**                          | **Описание**                                                        | **Значение по умолчанию**     ||
    || `dataproc:spark_driver_memory_fraction` (`drMemF`) | Доля памяти вычислительного хоста, резервируемая для драйвера       | `0.25`                        ||
    || `dataproc:spark_executors_per_vm` (`numCon`)       | Максимальное количество контейнеров на одном вычислительном хосте   | `2`                           ||
    || `spark:spark.executor.cores` (`exCPU`)             | Количество ядер процессора, выделяемых для каждого исполнителя      | `allCPU` / `numCon`           ||
    || `spark:spark.executor.memory` (`exMem`)            | Объем памяти (МБ), выделяемый на каждого исполнителя                | `nmMem` / `numCon`            ||
    || `spark:spark.driver.cores` (`drCPU`)               | Количество ядер процессора, выделяемых для YARN Application Master  | `allCPU` / `numCon`           ||
    || `spark:spark.driver.memory` (`drMem`)              | Объем памяти (МБ), выделяемый для YARN Application Master           | `drMemF` × `nmMem` / `numCon` ||
    |#

{% endlist %}

Значения по умолчанию, установленные в сервисе, оптимальны для запуска одного приложения. Чтобы оптимизировать распределение ресурсов под свои задачи, измените режим размещения драйвера и другие свойства компонентов:

* Для всех новых заданий в кластере:

    * при [создании кластера](../operations/cluster-create.md);
    * при [изменении кластера](../operations/cluster-update.md).

* Для отдельного задания при его [создании](../operations/jobs-spark.md#create).

### Примеры {#examples}

На кластере с настройками по умолчанию и двумя вычислительными хостами запущено одно приложение. В этом случае:

{% list tabs %}

- deployMode=client

    * Драйвер может занять все ресурсы управляющего хоста.
    * Количество ресурсов, доступное исполнителям на всех вычислительных хостах, будет снижено на величину, зарезервированную для YARN Application Master.
    * Ресурсы, зарезервированные для YARN Application Master на втором хосте, останутся неиспользованы.

    ![lightweight-load](../../_assets/data-proc/lightweight-load.svg)

- deployMode=cluster

    * На управляющем хосте может быть запущен HDFS или другая требовательная к ресурсам программа.
    * YARN Application Master с драйвером займет существенную часть ресурсов на одном из вычислительных хостов, но не больше, чем размер контейнера для исполнителей. Из-за этого часть ресурсов может остаться неиспользованной.
    * На втором вычислительном хосте оба контейнера достанутся исполнителям.

    ![heavyweight-load](../../_assets/data-proc/heavyweight-load.svg)

{% endlist %}

## Полезные ссылки {#see-also}

В документации Apache Spark приведены подробные сведения:

* [о настройках Spark-приложений](https://spark.apache.org/docs/latest/configuration.html);
* [о режимах работы Spark-драйвера](https://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn);
* [о выделении ресурсов для Spark-приложения](https://spark.apache.org/docs/latest/running-on-yarn.html#resource-allocation-and-configuration-overview).

Чтобы проверить работу Spark-приложений в кластере {{ dataproc-name }}, [проведите мониторинг состояния Spark-приложений](../operations/spark-monitoring.md).

Если Spark-приложение работает медленно, [проведите первичную диагностику производительности](../operations/spark-diagnostics.md#diagnostics).
