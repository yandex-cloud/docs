# Высокопроизводительные вычисления (HPC) на прерываемых виртуальных машинах

[HPC-кластеры](https://en.wikipedia.org/wiki/Computer_cluster) используются в вычислительных целях, в частности в научных исследованиях и расчетных задачах. Вычислительный кластер представляет собой массив серверов (вычислительных узлов), объединенных сетью. Каждый вычислительный узел имеет несколько многоядерных процессоров, свою оперативную память и работает под управлением независимой операционной системы. Наиболее распространено использование однородных кластеров, где все узлы одинаковы по своей архитектуре и производительности.

По этой инструкции вы создадите кластер [прерываемых виртуальных машин](../../compute/concepts/preemptible-vm.md), которые будут решать общую вычислительную задачу. Примером вычислительной задачи будет решение системы линейных алгебраических уравнений с помощью [метода Якоби](https://ru.wikipedia.org/wiki/Метод_Якоби).

Для создания кластера и запуска вычислительной задачи:

1. [Подготовьте облако к работе](#before-you-begin).
1. [Подготовьте основную виртуальную машину в облаке](#create-master-vm).
   1. [Создайте виртуальную машину](#create-vm).
   1. [Настройте виртуальную машину](#setup-vm).
1. [Подготовьте кластер виртуальных машин](#prepare-cluster).
1. [Создайте кластер](#create-cluster).
   1. [Проверьте работу кластера](#test-cluster).
   1. [Настройте NFS](#configure-nfs).
   1. [Смонтируйте директории на ВМ из группы](#mount).
1. [Подготовьте задачу для вычислений в кластере](#config-hpc).
1. [Запустите и проанализируйте вычисления](#start-hpc).
1. [Как удалить созданные ресурсы](#clear-out).


## Подготовьте облако к работе {#before-you-begin}

{% include [before-you-begin](../_tutorials_includes/before-you-begin.md) %}

{% if product == "yandex-cloud" %}

### Необходимые платные ресурсы {#paid-resources}

В стоимость поддержки серверов входит:

* плата за несколько запущенных виртуальных машин (см. [тарифы {{ compute-full-name }}](../../compute/pricing.md));
* плата за использование динамического или статического внешнего IP-адреса (см. [тарифы {{ vpc-full-name }}](../../vpc/pricing.md));


{% endif %}

## Подготовьте основную виртуальную машину в облаке {#create-master-vm}

### Создайте виртуальную машину {#create-vm}

Чтобы создать виртуальную машину:

1. На странице каталога в [консоли управления]({{ link-console-main }}) нажмите кнопку **Создать ресурс** и выберите пункт **Виртуальная машина**.
1. В поле **Имя** введите имя виртуальной машины. Для наглядности примера укажите `master-node`.
1. Выберите [зону доступности](../../overview/concepts/geo-scope.md), в которой должна находиться виртуальная машина.
1. В блоке **Выбор образа/загрузочного диска** перейдите на вкладку **{{ marketplace-name }}** и выберите образ [Ubuntu](/marketplace?tab=software&search=Ubuntu&categories=os).
1. В блоке **Диски** выберите жесткий диск размером 13 ГБ. Тип диска выберите **SSD**, поскольку он будет использоваться для сетевого доступа другими виртуальными машинами.
1. В блоке **Вычислительные ресурсы**:
   - Выберите [платформу](../../compute/concepts/vm-platforms.md) виртуальной машины.
   - Для решения текущих вычислительных задач укажите конфигурацию:
    - **Платформа** — Intel Ice Lake.
    - **Гарантированная доля vCPU** — 100%.
    - **vCPU** — 4.
    - **RAM** — 4 ГБ.
    - **Дополнительно** — «Прерываемая».

1. В блоке **Сетевые настройки**:
   - Выберите **Сеть** и **Подсеть**, к которым нужно подключить виртуальную машину. Если нужной сети или подсети еще нет, вы можете создать их прямо на странице создания ВМ.
   - В поле **Публичный адрес** оставьте значение **Автоматически**, чтобы назначить виртуальной машине случайный внешний IP-адрес из пула {{ yandex-cloud }}, или выберите статический адрес из списка, если вы зарезервировали его заранее.

1. В блоке **Доступ** укажите данные для доступа к виртуальной машине:
   - В поле **Логин** введите предпочтительное имя пользователя, который будет создан на виртуальной машине.
   - В поле **SSH-ключ** скопируйте ваш открытый SSH-ключ. Пару ключей для подключения по SSH необходимо создать самостоятельно, см. [раздел о подключении к виртуальным машинам по SSH](../../compute/operations/vm-connect/ssh.md).

1. Нажмите кнопку **Создать ВМ**.

### Настройте виртуальную машину {#setup-vm}

1. [Зайдите по SSH](../../compute/operations/vm-connect/ssh.md) на виртуальную машину и перейдите в режим администратора в консоли:
   ```bash
   sudo -i
   ```

1. Выполните обновление репозитория и поставьте требуемые утилиты:
   ```bash
   apt update
   apt install -y net-tools htop libopenmpi-dev nfs-common
   ```

1. Выйдите из режима администратора и сгенерируйте SSH-ключи для доступа между виртуальными машинами:
   ```bash
   exit
   ssh-keygen -t ed25519
   ```

1. Добавьте сгенерированный ключ в список разрешенных:
   ```bash
   cd ~/.ssh
   cat id_ed25519.pub >> authorized_keys
   ```

## Подготовьте кластер виртуальных машин {#prepare-cluster}

### Создайте кластер {#create-cluster}

1. В [консоли управления]({{ link-console-main }}) перейдите в раздел **Диски** и нажмите **Создать снимок** у диска ВМ `master-node`. Задайте имя `master-node-snapshot`. После того, как снимок будет создан, он появится в разделе **Снимки дисков**.
1. Перейдите в раздел **Группы виртуальных машин** и нажмите **Создать группу**.
1. Создайте группу виртуальных машин:
   - В поле **Имя** укажите имя будущей группы, например `compute-group`.
   - В поле **Сервисный аккаунт** добавьте [сервисный аккаунт](../../compute/concepts/instance-groups/access.md) к данной группе. Если у вас нет сервисного аккаунта, нажмите **Создать новый**, укажите его имя и нажмите на кнопку **Создать**.
   - Выберите ту же **Зону доступности** в которой находится ВМ `master-node`. Зона должна совпадать у виртуальных машин, чтобы минимизировать задержки при их взаимодействии.
   - В блоке **Шаблон виртуальной машины** нажмите кнопку **Задать**. Откроется экран создания шаблона.
     - В блоке **Диски** выберите **Добавить диск**. В открывшемся окне укажите: 

       - **Назначение диска** — загрузочный. 
       - **Тип диска** — SSD. 
       - **Наполнение** — из созданного снимка `master-node-snapshot`. 

     - В блоке **Вычислительные ресурсы**  задайте конфигурацию, аналогичную конфигурации основной виртуальной машины:

       - **Платформа** — Intel Ice Lake.
       - **Гарантированная доля vCPU** — 100%.
       - **vCPU** — 4.
       - **RAM** — 4 ГБ.
       - **Дополнительно** — «Прерываемая».
     
     - В блоке **Сетевые настройки** укажите те же сеть и подсеть, что и у основной машины. Тип адреса оставьте **Автоматически**.
     - В блоке **Доступ** укажите данные для доступа к виртуальной машине:
       - В поле **Логин** введите предпочтительное имя пользователя, который будет создан на виртуальной машине.
       - В поле **SSH-ключ** скопируйте ваш открытый SSH-ключ. Пару ключей для подключения по SSH необходимо создать самостоятельно, см. [раздел о подключении к виртуальным машинам по SSH](../../compute/operations/vm-connect/ssh.md).
     - Нажмите кнопку **Добавить**. Вы вернетесь на экран создания группы ВМ.
1. В блоке **Масштабирование** выберите количество создаваемых виртуальным машин. Укажите 3 виртуальные машины. 
1. Нажмите кнопку **Создать**.

### Проверьте работу кластера {#test-cluster}

[Зайдите по SSH](../../compute/operations/vm-connect/ssh.md) на каждую из виртуальных машин в группе `compute-group` и убедитесь, что с них есть доступ к ВМ `master-node` по SSH.

```bash
ping master-node
ssh master-node
```

### Настройте NFS {#configure-nfs}

Чтобы виртуальные машины могли использовать одни и те же исходные файлы, создайте общую сетевую директорию с помощью протокола сетевого доступа [NFS](https://ru.wikipedia.org/wiki/Network_File_System):

1. Зайдите на ВМ `master-node` по SSH и установите NFS-сервер:

   ```bash
   ssh <Публичный IP-адрес ВМ master-node>
   sudo apt install nfs-kernel-server
   ```

1. Создайте директорию `shared`, которая будет общей для всех виртуальных машин:

   ```bash
   mkdir ~/shared
   ```

1. Откройте файл `/etc/exports` любым текстовым редактором, например, `nano`:
   
   ```bash
   sudo nano /etc/exports
   ```

1. Добавьте запись для доступа к директории `shared` в файл:

   ```
   /home/<имя пользователя>/shared *(rw,sync,no_root_squash,no_subtree_check)
   ```
   
   Сохраните файл.

1. Примените настройки и перезагрузите сервис:

   ```bash
   sudo exportfs -a
   sudo service nfs-kernel-server restart
   ```

#### Смонтируйте директории на ВМ из группы {#mount}

На каждой ВМ из группы `compute-group` смонтируйте созданную директорию:

1. Создайте директорию `shared` и смонтируйте туда директорию с ВМ `master-node`:
   ```bash
   mkdir ~/shared
   sudo mount -t nfs master-node:/home/<имя пользователя>/shared ~/shared
   ```

1. Убедитесь, что директория была успешно смонтирована:
   ```bash
   df -h
   Filesystem                            Size  Used Avail Use% Mounted on
   ...
   master-node:/home/<имя пользователя>/shared   13G  1.8G   11G  15% /home/<имя пользователя>/shared
   ```

## Подготовьте задачу для вычислений в кластере {#config-hpc}

1. Зайдите по SSH на ВМ `master-node`, перейдите в директорию `shared` и скачайте исходный файл `task.c` с вычислительной задачей:
   ```bash
   cd ~/shared
   wget https://raw.githubusercontent.com/cloud-docs-writer/examples/master/hpc-on-preemptible/task.c
   ```

   Этот код решает систему линейных алгебраических уравнений с помощью [метода Якоби](https://ru.wikipedia.org/wiki/Метод_Якоби). Задача имеет одну из распределенных реализаций с помощью MPI.

1. Скомпилируйте исходный файл в исполняемый:
   ```bash
   mpicc task.c -o task
   ```
   В директории `shared` должен был появиться исполняемый файл `task`.

## Запустите и проанализируйте вычисления {#start-hpc}

{% note tip %}

Для проверки загрузки ядер виртуальных машин можно выполнять команду `htop` в отдельной SSH-сессии на каждой виртуальной машине.

{% endnote %}

1. Запустите выполнение задачи на 2 ядрах, используя ресурсы только ВМ `master-node`:
   ```bash
   mpirun -np 2 task
   ```

   После выполнения задачи программа выведет затраченное на решение время:
   ```
   JAC1 STARTED
   1: Time of task=45.104153
   0: Time of task=45.103931
   ```

1. Запустите выполнение задачи на 4 ядрах, используя ресурсы только ВМ `master-node` и получите соответствующие результаты:
   ```bash
   mpirun -np 4 task
   JAC1 STARTED
   1: Time of task=36.562328
   2: Time of task=36.562291
   3: Time of task=36.561989
   0: Time of task=36.561695
   ```

1. Запустите выполнение задачи на 4 ядрах, используя ресурсы двух виртуальных машин, по 2 ядра на каждой машине. Для этого запустите выполнение задачи с ключом `-host`, который принимает параметры вида `<IP-адрес ВМ>:<количество ядер>[,<ip>:<cores>[,...]]`. После вычисления задачи программа выведет результат:
   ```bash
   mpirun -np 4 -host localhost:2,<IP-адрес ВМ>:2 task
   JAC1 STARTED
   0: Time of task=24.539981
   1: Time of task=24.540288
   3: Time of task=24.540619
   2: Time of task=24.540781
   ```

1. По аналогии можно продолжать увеличивать число используемых виртуальных машин и ядер и убедиться, что распределенные вычисления позволяют значительно увеличивать скорость выполнения задачи.


## Как удалить созданные ресурсы {#clear-out}

Чтобы перестать платить за развернутый сервер и группу виртуальных машин, достаточно удалить ВМ `master-node` и группу `compute-group`.

Если вы зарезервировали статический публичный IP-адрес специально для этой ВМ:

1. Выберите сервис **{{ vpc-short-name }}** в вашем каталоге.
1. Перейдите на вкладку **IP-адреса**.
1. Найдите нужный адрес, нажмите значок ![ellipsis](../../_assets/options.svg) и выберите пункт **Удалить**.
