# Configuring a fault-tolerant architecture in {{ yandex-cloud }}

In this guide, you'll learn how to configure a fault-tolerant architecture in {{ yandex-cloud }} and check how it works using various test cases.

Fault tolerance is the property that enables a system to continue operating after a failure of one or more of its components.

To configure and test the architecture:

1. [Before you start](#before-begin).
1. [Set up a test bench](#prepare):
    - [Create TodoList app containers](#create-app).
    - [Deploy the infrastructure](#create-environment).
    - [Create and run the Load Testing Tool app](#create-load-testing-tool).
1. [Run the following test scenarios](#run):
    - [VM failure](#error-vm).
    - [Application failure](#error-app).
    - [Availability zone failure](#zone-down).
    - [Updating an application](#update-app).
    - [Scaling a DB configuration](#scaling-database).

If you no longer need the created resources, [delete them](#clear-out).

## Before you start {#before-begin}

{% include [before-you-begin](../_tutorials_includes/before-you-begin.md) %}


### Required paid resources {#paid-resources}

The cost for supporting a fault-tolerant {{ yandex-cloud }} architecture includes:

- A fee for the disks and continuously running VMs (see [{{compute-full-name}} pricing](../../compute/pricing.md)).
- A fee for the continuously running {{ mpg-name }} cluster (see [pricing for {{mpg-full-name}}](../../managed-postgresql/pricing.md)).
- A fee for using a dynamic or static external IP address (see [{{vpc-full-name}} pricing](../../vpc/pricing.md)).


## Set up a test bench {#prepare}

### Test bench description {#stand}

Description of the test bench:

- The application is packaged into a Docker image and pushed to {{container-registry-name}}.

    Docker images are deployed on four VMs based on a {{coi}}. The VMs are grouped and located in two different availability zones.

- A DB cluster is managed by {{ mpg-short-name }} and consists of two hosts that reside in different availability zones.

- The load is generated by the [Load Testing Tool](/marketplace/products/yc/load-testing) app from {{ marketplace-name }} and goes to {{network-load-balancer-short-name}}. The load balancer distributes traffic across VMs.

### Create TodoList app containers {#create-app}

To prepare the application to run in {{ yandex-cloud }}:

1. Authenticate with {{container-registry-name}}:

    ```bash
    yc container registry configure-docker
    ```

1. Create a registry:

    ```bash
    yc container registry create --name todo-registry
    ```

1. Build a Docker image with the `v1` tag:

    ```bash
    docker build . --tag cr.yandex/<registry_id>/todo-demo:v1
    ```

1. Build a Docker image with the `v2` tag (to test the application update case):

    ```bash
    docker build . --build-arg COLOR_SCHEME=dark --tag cr.yandex/<registry_id>/todo-demo:v2
    ```

1. Push the Docker images to {{container-registry-name}}:

    ```bash
    docker push cr.yandex/<registry_id>/todo-demo:v1
    docker push cr.yandex/<registry_id>/todo-demo:v2
    ```

### Deploy the infrastructure {#create-environment}

To prepare the environment for running the application in {{ yandex-cloud }}:

1. Install [{{ TF }}](https://www.terraform.io).

1. Download a [repository](https://github.com/glebmish/yandex-cloud-fault-tolerance-demo/archive/master.zip) with the demo app source code, {{ TF }} specs, and a script to simulate app failure.

1. Go to the directory with the environment specification:

    ```bash
    cd app
    ```

1. Initialize {{ TF }} in the spec directory:

    ```bash
    terraform init
    ```

1. In a file named `app/todo-service.tf`, specify the path to the public SSH key (the default value is `~/.ssh/id_rsa.pub`).

1. Deploy and run the application:

    ```bash
    terraform apply -var yc_folder=<folder_id> -var yc_token=<yc_token> -var user=$USER
    ```

    Where:

    * `folder_id`: The folder where the application will be deployed.
    * `yc_token`: The OAuth-token of the user that you want to deploy the application under.

The following resources are created:

- A {{vpc-short-name}} network with three subnets in all availability zones.
- Two service accounts:
  - A service account for managing a VM instance group with the `editor` role.
  - A service account for pulling a Docker image to a VM with the `container-registry.images.puller` role.
- An instance group of four {{coi}}-based VM instances in the `{{ region-id }}-b` and `{{ region-id }}-c` availability zones.
- A {{mpg-short-name}} cluster with two hosts in the `{{ region-id }}-b` and `{{ region-id }}-c` availability zones.
- A network load balancer for distributing traffic across VM instances in the instance group.

To access the application, go to the `lb_address` received after executing the `terraform apply` command.

### Create and run the Load Testing Tool app {#create-load-testing-tool}

{% note warning %}

Before creating your Load Testing Tool app, [create TodoList app containers](#create-app) and [deploy the infrastructure](#create-environment).

{% endnote %}

1. Go to the directory with the Load Testing Tool specification:

    ```bash
    cd load-testing-tool
    ```

1. Initialize {{ TF }} in the Load Testing Tool spec directory:

    ```bash
    terraform init
    ```

1. In a file named `load-testing-tool/main.tf`, specify the path to the public and private SSH keys (the default values are `~/.ssh/id_rsa.pub` and `~/.ssh/id_rsa`).

1. Deploy and run the VM:

    ```bash
    terraform apply -var yc_folder=<folder_id> -var yc_token=<yc_token> -var user=$USER -var overload_token=<overload_token>
    ```

    Where:

    * `folder_id`: The folder where the Load Testing Tool app will be deployed.
    * `yc_token`: The OAuth-token of the user that you want to deploy the Load Testing Tool app under.
    * `overload_token`: The token that is used to connect to `<overload.yandex.net>`. To get the token, log in, click on your profile at the top right, and select **My api token** from the drop-down menu.

1. Connect to the VM over SSH. The connection address is specified in the `terraform apply` command output.

1. Run the Load Testing Tool app:

    ```bash
    sudo yandex-tank -c load.yaml
    ```

1. Go to `<overload.yandex.net>` and find the running shot: **Public tests** -> **show my tests only**.

## Running scenarios {#run}

### VM failure {#error-vm}

How the failure shows itself: The VM with the application is unavailable.

Possible cause:

- A failure of the physical host that the VM was running on.
- The VM with the application was deleted by mistake.

To simulate the failure, delete one of the instances in the group.

Test bench reaction:

1. The network load balancer and {{ ig-name }} get information about the VM failure and exclude it from load balancing: traffic stops coming to this VM instance and is distributed across the remaining instances in the group.
1. {{ ig-name }} [is automatically restored](../../compute/concepts/instance-groups/autohealing.md):
   1. Deletes the unavailable VM instance (in this test case, it is already deleted and this step is skipped).
   1. Creates a new VM instance.
   1. Waits for the application to start on the machine.
   1. Adds the VM instance to load balancing.

The load balancer and {{ ig-name }} require some time to detect the problem and disable traffic to the faulty VM instance. This may cause Connection Timeout errors (HTTP code `0` in the **Quantities** and **HTTP codes** charts in the Load Testing Tool monitoring tool).

After disabling load balancing for the unavailable VM instance, the user load is handled correctly.

### Application failure {#error-app}

How the failure shows itself: The application doesn't respond in time or doesn't work correctly from the user's point of view.

Possible cause:

- A memory leak caused the application to fail.
- The application is unable continue due to DB connectivity loss.
- The application fails to handle requests due to heavy load.

According to [health check](../../compute/concepts/instance-groups/autohealing.md#setting-up-health-checks) settings, {{ ig-name }} polls VM instances in the group over HTTP. When operating normally, accessing the `/healthy` endpoint returns the HTTP code `200`. Otherwise, {{ ig-name }} starts the recovery procedure.

To simulate the failure, run the script:

```bash
fail_random_host.sh <group_id>
```

A random instance from the group will start returning the HTTP code `503`.

Test bench reaction:

1. {{ ig-name }} receives information about the application failure and excludes the VM instance from load balancing: traffic stops coming to this instance and is distributed across the remaining instances in the group.
1. {{ ig-name }} [is automatically restored](../../compute/concepts/instance-groups/autohealing.md):
   1. Restarts the faulty VM instance.
   1. Waits for the application to start on the machine.
   1. Adds the VM instance to load balancing.

{{ ig-name }} polls the instance several times before disabling traffic and starting the recovery procedure. This may cause Service Unavailable errors (HTTP code `503` in the **Quantities** and **HTTP codes** charts in the Load Testing Tool monitoring tool).

After disabling load balancing for the faulty VM instance, the user load is handled correctly.

### Availability zone failure {#zone-down}

How the failure shows itself: Multiple VMs are unavailable in the same  zone.

Possible cause:

- Data center downtime.
- Scheduled maintenance in the data center.

To move your resources to another data center:

1. In the [management console]({{ link-console-main }}), select the folder with your instance group.
1. From the list of services, select {{compute-name}}.
1. Click **Instance groups**.
1. Select the `todo-ig` group.
1. Click **Edit**.
1. In the **Distribution** section, uncheck the `{{ region-id }}-c` availability zone.
1. Click **Save changes**.

Test bench reaction:

1. {{ ig-name }} disables load balancing for the VMs in the `{{ region-id }}-c` availability zone.
1. These VMs are deleted and, at the same time, VMs are created in the `{{ region-id }}-b` zone.
1. {{ ig-name }} adds the created VMs to load balancing.

The number of VM instances that can be simultaneously created and deleted depends on the [deployment policy](../../compute/concepts/instance-groups/policies/deploy-policy.md).

While disabling load balancing for VM instances, Connection Timeout errors may occur (HTTP code `0` in the **Quantities** and **HTTP codes** charts in the Load Testing Tool monitoring tool).

After load balancing is disabled for the VM instances, the user load is handled correctly.

### Updating an application {#update-app}

To update the application:

1. In the [management console]({{ link-console-main }}), select the folder with your instance group.
1. From the list of services, select {{compute-name}}.
1. Click **Instance groups**.
1. Select the `todo-ig` group.
1. Click **Edit**.
1. In the **Instance template** section, click ![horizontal-ellipsis](../../_assets/horizontal-ellipsis.svg) and select **Edit**.
1. On the **Container Solution** tab, select the required Docker container.
1. In the **Docker image** field of the window that opens, enter the name of the Docker image with the new app version.
1. Click **Apply**.
1. Click **Save**.
1. Click **Save changes**.

Test bench reaction:

1. {{ ig-name }} disables load balancing for two VM instances with the outdated app version (the [status](../../compute/concepts/instance-groups/statuses.md#vm-statuses) of these instances is `RUNNING_OUTDATED`).
1. Deletes them while creating VM instances with the new app version.
1. Adds the created VM instances to load balancing.
1. The actions are repeated for the remaining two instances with the outdated app version.

Refresh the app page. If the network load balancer sends your request to a VM instance that's already updated, you'll see the app version with a dark color scheme.

The number of VM instances that can be simultaneously created and deleted depends on the [deployment policy](../../compute/concepts/instance-groups/policies/deploy-policy.md).

While disabling load balancing for VM instances, Connection Timeout errors may occur (HTTP code `0` in the **Quantities** and **HTTP codes** charts in the Load Testing Tool monitoring tool).

After load balancing is disabled for the VM instances, the user load is handled correctly.

### Scaling a DB configuration {#scaling-database}

You may need to scale your DB if:

- Cluster host performance is insufficient to handle requests.
- The data requires more storage capacity.

To scale the DB configuration:

1. In the [management console]({{ link-console-main }}), select the folder with your DB cluster.
1. From the list of services, select {{mpg-short-name}}.
1. Select the `todo-postgresql` cluster.
1. Click **Edit cluster**.
1. Under **Host class**, select **s2.medium**.
1. Click **Save changes**.

{{mpg-short-name}} will run the update command for the cluster.

When switching between the master and a replica (at the beginning and end of the update process), an Internal Server Error may occur (HTTP code `500` in the **Quantities** and **HTTP codes** charts in the Load Testing Tool monitoring tool).

After switching, the user load is handled correctly.

## Deleting applications and environments {#clear-out}

{% note warning %}

If a VM with Load Testing Tool is created, make sure you delete it first, otherwise deleting the {{ vpc-short-name }} will fail.

{% endnote %}

To delete the Load Testing Tool app, go to the `load-testing-tool` directory and run the following command:

```bash
terraform destroy -var yc_folder=<folder_id> -var yc_token=<yc_token> -var user=$USER -var overload_token=not-used
```

To delete the TodoList app, go to the `app` directory and run the following command:

```bash
terraform destroy -var yc_folder=<folder_id> -var yc_token=<yc_token> -var user=$USER
```
