* `--security-group-ids`: List of [security group](../../../../managed-airflow/concepts/network.md#security-groups) IDs.
* `--webserver`, `--scheduler`, `--worker`, `--triggerer`, `--dag-processor`: {{ maf-name }} [component](../../../../managed-airflow/concepts/index.md#components) configuration:

    * `count`: Number of instances in the cluster for the web server, scheduler, DAG processor, and Triggerer.
    * `min-count`, `max-count`: Minimum and maximum number of instances in the cluster for the worker.
    * `resource-preset-id`: [ID of the computing resources](../../../../managed-airflow/concepts/index.md#presets) of the web server, scheduler, DAG processor, worker, and Triggerer. The possible values are:

        * `c1-m2`: 1 vCPU, 2 GB RAM
        * `c1-m4`: 1 vCPU, 4 GB RAM
        * `c2-m4`: 2 vCPUs, 4 GB RAM
        * `c2-m8`: 2 vCPUs, 8 GB RAM
        * `c4-m8`: 4 vCPUs, 8 GB RAM
        * `c4-m16`: 4 vCPUs, 16 GB RAM
        * `c8-m16`: 8 vCPUs, 16 GB RAM
        * `c8-m32`: 8 vCPUs, 32 GB RAM

    {% include notitle [dag-processor](../dag-processor.md) %}

* `--deb-packages`, `--pip-packages`: Lists of deb and pip packages enabling you to install additional libraries and applications in the cluster for running DAG files:

    You can set version restrictions for the installed packages, e.g.:

    ```hcl
    --pip-packages "pandas==2.0.2,scikit-learn>=1.0.0,clickhouse-driver~=0.2.0"
    ```

    The package name format and version are defined by the install command: `pip install` for pip packages and `apt install` for deb packages.

* `--dags-bucket`: Name of the bucket to store DAG files in.

* `--maintenance-window`: [Maintenance window](../../../../managed-airflow/concepts/maintenance.md) settings (including for disabled clusters), where `type` is the maintenance type:

    {% include [maintenance-window](../../../../_includes/mdb/cli/maintenance-window-description.md) %}

* `--deletion-protection`: Enables cluster protection against accidental deletion.

    Even if it is enabled, one can still connect to the cluster manually and delete it.

* `--lockbox-secrets-backend`: Enables using secrets in [{{ lockbox-full-name }}](../../../../lockbox/concepts/index.md) to [store {{ AF }} configuration data, variables, and connection parameters](../../../../managed-airflow/concepts/impersonation.md#lockbox-integration).
* `--airflow-config`: [{{ AF }}](https://airflow.apache.org/docs/apache-airflow/2.2.4/configurations-ref.html) additional properties. Provide them in `<configuration_section>.<key>=<value>` format, such as the following:

    ```bash
    --airflow-config core.load_examples=False
    ```

* Logging parameters:

    * `--log-enabled`: Enables logging. Logs generated by {{ AF }} components will be sent to {{ cloud-logging-full-name }}.
    * `--log-folder-id`: Folder ID. Logs will be written to the default [log group](../../../../logging/concepts/log-group.md) for this folder.
    * `--log-group-id`: Custom log group ID. Logs will be written to this group.

        Specify one of the two parameters: `--log-folder-id` or `--log-group-id`.

    * `--log-min-level`: Minimum logging level. Possible values: `TRACE`, `DEBUG`, `INFO` (default), `WARN`, `ERROR`, and `FATAL`.
