# Configuring S3 source endpoint

When [creating](../index.md#create) or [updating](../index.md#update) an endpoint, configure access to S3-compatible storage.

## Settings {#settings}

{% list tabs %}

- Management console

   * **Dataset**: Specify the name of an auxiliary table that will be used for the connection.
   * **Path Pattern**: Enter the path pattern. If the bucket only includes files, use the value `**`.
   * **Schema**: Specify JSON schema in the format `{"column": "data type"}`. Use the value `{}` for automatic schema detection based on files.
   * **Format**: Select the format that corresponds to your files: `csv` or `parquet`.

      * **csv**: specify the csv-files settings:

         * **Delimiter**: Delimiter character.
         * **Quote Char**: A character used to escape reserved characters.
         * **Escape Char**: Escape-character.
         * **Encoding**: [Encoding](https://docs.python.org/3/library/codecs.html#standard-encodings).
         * **Double Quote**: Enable this option to replace double quotes with single quotes.
         * **Newlines In Values**: Enable the option if your text data values might include newline characters.
         * **Block Size**: Size of a data chunk used to read data from files, in bytes.
         * **Additional Reader Options**: Required CSV [ConvertOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions) to edit. Specified as a JSON-string.
         * **Advanced Options**: Required CSV [ReadOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions) to edit. Specified as a JSON-string.

      * **parquet**: Specify parquet-files settings:

         * **Buffer Size**: Size of the buffer used to deserialize specific parts of columns.
         * **Columns**: Columns for reading data. Leave this field empty to read all the columns.
         * **Batch Size**: A maximum number of records in a batch.

   * **S3: Amazon Web Services**: Specify settings for the S3 provider:

      * **Bucket**: Bucket name.
      * **AWS Access Key Id** and **AWS Secret Access Key**: [ID and contents of the AWS key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) used to access a private bucket.
      * (Optional) **Path Prefix**: Prefix for folders and files that shouldn't be processed by AWS.
      * (Optional) **Endpoint**: Services that need to be used but aren't compatible with Amazon S3. Leave this field empty to use the Amazon service.
      * **Use SSL**: Enable to use custom servers over HTTPS. Ignored when using the Amazon service.
      * **Verify SSL Cert**: Enable to skip authentication of the server's SSL certificate. This setting is useful if you use self-signed certificates. Ignored when using the Amazon service.

{% endlist %}

For more information about settings, see the [Airbyte® documentation](https://docs.airbyte.com/integrations/sources/s3).

{% include [airbyte-trademark](../../../../_includes/data-transfer/airbyte-trademark.md) %}
