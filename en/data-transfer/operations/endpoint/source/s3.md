---
title: Transferring data from an S3 source endpoint
description: Follow this guide to set up a data transfer from an S3 source endpoint.
---

# Transferring data from an S3 source endpoint

{{ data-transfer-full-name }} enables you to migrate data from S3 storage to {{ yandex-cloud }} managed databases and implement various data processing and transformation scenarios. To implement a transfer:

1. [Explore possible data transfer scenarios](#scenarios).
1. [Prepare the S3 database](#prepare) for the transfer.
1. [Set up a source endpoint](#endpoint-settings) in {{ data-transfer-full-name }}.
1. [Set up one of the supported data targets](#supported-targets).
1. [Create](../../transfer.md#create) a transfer and [start](../../transfer.md#activate) it.
1. In case of any issues, [use ready-made solutions](../../../../data-transfer/troubleshooting/index.md) to resolve them.

## Scenarios for transferring data from S3 {#scenarios}

You can implement scenarios for data migration and delivery from the Amazon Simple Storage Service (S3) storage to managed databases for further storage in the cloud, processing and loading into data marts for further visualization.

For a detailed description of possible {{ data-transfer-full-name }} scenarios, see [Tutorials](../../../tutorials/index.md).

## Preparing the S3 database {#prepare}

{% include [prepare s3 db](../../../../_includes/data-transfer/endpoints/sources/s3-prepare.md) %}

## Settings {#settings}

When [creating](../index.md#create) or [updating](../index.md#update) an endpoint, configure access to S3-compatible storage.

{% list tabs group=instructions %}

- Management console {#console}

    * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.dataset.title }}**: Specify the name of an auxiliary table that will be used for the connection.
    * **Path Pattern**: Enter the path pattern. If the bucket contains nothing but files, use the `**` value.
    * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.schema.title }}**: Specify the JSON schema in `{"<column>": "<data_type>"}` format. Use the `{}` value for automatic schema detection based on files.
    * **format**: Select the format matching your files: `CSV`, `parquet`, `Avro`, or `JSON Lines`.

        * **CSV**: Specify the settings of CSV files:

            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.delimiter.title }}**: Delimiter character.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.quote_char.title }}**: Character used to escape reserved characters.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.escape_char.title }}**: Character used to escape special characters.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.encoding.title }}**: [Encoding](https://docs.python.org/3/library/codecs.html#standard-encodings).
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.double_quote.title }}**: Enable this option to replace double quotes with single quotes.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.newlines_in_values.title }}**: Enable the option if your text data values might includeÂ newline characters.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.block_size.title }}**: Size of a data chunk used to read data from files, in bytes.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.additional_reader_options.title }}**: Required CSV [ConvertOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions) to edit, which are specified as a JSON-string.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Csv.advanced_options.title }}**: Required CSV [ReadOptions](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions) to edit, which are specified as a JSON-string.

        * **parquet**: Specify parquet-files settings:

            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Parquet.buffer_size.title }}**: Size of the buffer used to deserialize specific parts of columns.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Parquet.columns.title }}**: Columns for reading data. Leave this field empty to read all the columns.
            * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Parquet.batch_size.title }}**: Maximum number of records in a batch.

        * **JSON Lines**: Specify the settings for JSON Lines:

            * **Allow newlines in values**: Enable this option to allow newlines in JSON values. This may affect the transfer speed.
            * **Unexpected field behavior**: Specify how to handle JSON fields outside the `explicit_schema` (if the field values are set). For more information, see the [PyArrow documentation](https://arrow.apache.org/docs/python/generated/pyarrow.json.ParseOptions.html).
            * **Block Size**: Specify the block size (in bytes) from each file to be handled in-memory simultaneously. If the value you set is too large, the `Out of memory` error may occur during the transfer.

    * **S3: Amazon Web Services**: Specify the S3 provider's settings:

        * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.bucket.title }}**: Bucket name.
        * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.aws_access_key_id.title }}** and **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.aws_secret_access_key.title }}**: [ID and contents of the AWS key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys) used to access a private bucket.
        * (Optional) **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.path_prefix.title }}**: Prefix for folders and files not to be processed by AWS.
        * (Optional) **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.endpoint.title }}**: Services to use that are not compatible with Amazon S3. Leave this field empty to use the Amazon service.
        * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.use_ssl.title }}**: Enable to use custom servers over HTTPS. It is ignored when using the Amazon service.
        * **{{ ui-key.yc-data-transfer.data-transfer.endpoint.airbyte.s3_source.endpoint.airbyte.s3_source.S3Source.Provider.verify_ssl_cert.title }}**: Enable to skip authentication of the server's SSL certificate. This setting is useful if you use self-signed certificates. It is ignored when using the Amazon service.

{% endlist %}

For more information about the settings, see the [{{ AB }} documentation](https://docs.airbyte.com/integrations/sources/s3).

{% include [airbyte-trademark](../../../../_includes/data-transfer/airbyte-trademark.md) %}


## Configuring the data target {#supported-targets}

Configure one of the supported data targets:

* [{{ MY }}](../target/mysql.md)
* [{{ MG }}](../target/mongodb.md)
* [{{ CH }}](../target/clickhouse.md)
* [{{ GP }}](../target/greenplum.md)
* [{{ ydb-full-name }}](../target/yandex-database.md)
* [{{ KF }}](../target/kafka.md)
* [{{ DS }}](../target/data-streams.md)
* [{{ PG }}](../target/postgresql.md)

For a complete list of supported sources and targets in {{ data-transfer-full-name }}, see [Available transfers](../../../transfer-matrix.md).

{% include [Internet access](../../../../_includes/data-transfer/notes/internet-access.md) %}

After configuring the data source and target, [create and start the transfer](../../transfer.md#create).